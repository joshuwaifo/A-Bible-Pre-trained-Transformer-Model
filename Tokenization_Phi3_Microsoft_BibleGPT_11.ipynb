{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshuwaifo/A-Bible-Pre-trained-Transformer-Model/blob/main/Tokenization_Phi3_Microsoft_BibleGPT_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM\n",
        "\n",
        "Large Deep Neural Network\n",
        "\n",
        "Machine Learning\n",
        "\n",
        "Deep Learning\n",
        "\n",
        "Model is trained to predict the next word in a text\n",
        "\n",
        "Next word/token prediction task"
      ],
      "metadata": {
        "id": "FO9bY05xdzO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1st stage pretraining\n",
        "\n",
        "# train model to predict next word in a sentence/text"
      ],
      "metadata": {
        "id": "aYmVlLpieBtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample 1:\n",
        "\n",
        "LLMs -> learn\n",
        "\n",
        "Sample 2:\n",
        "\n",
        "LLMs learn -> to\n",
        "\n",
        "...\n",
        "\n",
        "Sample n:\n",
        "\n",
        "LLMs learn to predict one word at a -> time"
      ],
      "metadata": {
        "id": "x0Mc6DLyeQwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batching\n",
        "\n",
        "# Putting multiple training input together (in a batch)\n",
        "# Usually batches have to have the same length\n",
        "# Due to their implementation as a tensor aka matrix\n",
        "# To be clear: same number of elements in the columns\n",
        "\n",
        "# So data preparation normally needed\n",
        "\n",
        "# Take fixed size inputs\n",
        "# Slide it over the text\n",
        "# Create a batch\n",
        "# Do such over the whole dataset\n",
        "# Normally input lengths of at least 256/1024+ tokens"
      ],
      "metadata": {
        "id": "SBWs-p2RekHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to generate multi-word outputs?\n",
        "\n",
        "Like ChatGPT\n",
        "\n",
        "Still one word prediction task per iteration\n",
        "\n",
        "We pass the output back into the LLM's input and repeat the process until an end of text token (special token) is generated (which is the cue to stop the generation input) or a specific number of tokens determined by the user or system\n",
        "\n",
        "This takes us from next word prediction task to generating longer outputs\n",
        "\n",
        "To be more precise: we are generating one token at a time not necessarily a word"
      ],
      "metadata": {
        "id": "DIF2y9XffRlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenisation\n",
        "\n",
        "# Input text\n",
        "\n",
        "# Tokenised text (ie based on white spaces or more complex)\n",
        "# By breaking down the text/sentence into individual word tokens or punctuations etc\n",
        "\n",
        "# We then get the token ids from these tokenised text\n",
        "# This is normally built using a vocabulary (all the unique words in the training dataset)\n",
        "# typically sorted by position in the alphabet etc\n",
        "\n",
        "# GPT originally used the BPE (Byte Pair Encoding) tokeniser\n",
        "# Llama originally used Sentence Piece (now uses a mix)\n",
        "\n",
        "# This helps in dealing with unknown words\n",
        "# Using \"subwords\"\n",
        "\n",
        "# For example instead of 1 token per word\n",
        "# It could be 4 tokens per word\n",
        "\n",
        "# Ideally we would like to represent all the words we can in the vocabulary\n",
        "# However if a user comes up with a new word, this would be problamatic for such a vocabulary\n",
        "# Hence this subword technique"
      ],
      "metadata": {
        "id": "YnS-ugrWfQ2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Simple) Datasets used for (Pre-)Training LLMs - relative sizes (trend going to more data due to scaling laws - still not saturated the performance of LLMs yet)\n",
        "\n",
        "\n",
        "\n",
        "GPT-3 trained on 500 billion tokens in 2020\n",
        "\n",
        "- Common Crawl\n",
        "- Book datasets\n",
        "- Wikipedia dataset\n",
        "\n",
        "Nowadays the detail is more cryptic\n",
        "\n",
        "Llama 1  trained on 1.4 trillion tokens\n",
        "- Common Crawl (3.3 Terabytes)\n",
        "- C4\n",
        "- Github\n",
        "- Wikipedia\n",
        "- Books\n",
        "- ArXiv\n",
        "- StackExchange (78 Gigabytes)\n",
        "\n",
        "Llama 2 trained on 2 trillion tokens\n",
        "\n",
        "Llama 3 trained on 15 trillion tokens\n",
        "\n",
        "Nowadays they use phrases generically like publicly available sources (maybe due to potential of being sued and avoiding lawsuits)\n",
        "\n",
        "Good practice:\n",
        "\n",
        "Train on data in the public domain that is not copyrighted"
      ],
      "metadata": {
        "id": "CHWwZorLhkwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Phi-3 model by Microsoft\n",
        "# Focused on smaller models with smaller amounts of data\n",
        "# To leave more capacity for reasoning (hypothesis)\n",
        "\n",
        "# Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone 2024"
      ],
      "metadata": {
        "id": "UDY0nAs3ighe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tjfC1P-yjxGw"
      }
    }
  ]
}