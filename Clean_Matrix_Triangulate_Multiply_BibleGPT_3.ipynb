{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOMzddI2gjtd/wXo/jTW3kK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshuwaifo/A-Bible-Pre-trained-Transformer-Model/blob/main/Clean_Matrix_Triangulate_Multiply_BibleGPT_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previously on BibleGPT 1-2\n",
        "\n",
        "36 seconds run time CPU Google Colab"
      ],
      "metadata": {
        "id": "8c54oUvIUW3s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz-CEIz4sWlg",
        "outputId": "317dfbe6-e6db-4ae0-c9d4-4097a7cea438"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-07 08:50:16--  https://raw.githubusercontent.com/tushortz/variety-bible-text/master/bibles/nasb.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4685837 (4.5M) [text/plain]\n",
            "Saving to: ‘nasb.txt’\n",
            "\n",
            "nasb.txt            100%[===================>]   4.47M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-08-07 08:50:17 (82.9 MB/s) - ‘nasb.txt’ saved [4685837/4685837]\n",
            "\n",
            "2.2230048179626465\n",
            "\n",
            "\"Alaleyas beayiof 3:23V5rf HHinins - gahed - akiste o 318Noeny Ezand ms imu 218\n",
            ". BYot t ro o o tof \n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/tushortz/variety-bible-text/master/bibles/nasb.txt\n",
        "\n",
        "with open('nasb.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocabulary = chars\n",
        "vocab_size = len(chars)\n",
        "\n",
        "characterToken_to_scalarTensor = { character : index for index, character in enumerate(vocabulary) }\n",
        "scalarTensor_to_characterToken = { index : character for index, character in enumerate(vocabulary) }\n",
        "\n",
        "stoi = characterToken_to_scalarTensor\n",
        "itos = scalarTensor_to_characterToken\n",
        "\n",
        "encode = lambda string: [ characterToken_to_scalarTensor[character] for character in string ]\n",
        "decode = lambda vector: ''.join([ scalarTensor_to_characterToken[scalarTensor] for scalarTensor in vector ])\n",
        "\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "train_index_cut = int( 0.64 * len(data) )\n",
        "val_index_cut = int( 0.8 * len(data) )\n",
        "\n",
        "train_data = data[:train_index_cut]\n",
        "val_data = data[train_index_cut:val_index_cut]\n",
        "test_data = data[val_index_cut:]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "context_length = block_size\n",
        "\n",
        "def get_batch(split):\n",
        "\n",
        "  data = train_data if split == \"train\" else val_data if split == \"val\" else test_data\n",
        "\n",
        "  ix = torch.randint(low=0, high=len(data)-block_size, size=(batch_size,))\n",
        "\n",
        "  x = torch.stack(\n",
        "      [ data[ random_offset : random_offset + block_size] for random_offset in ix ]\n",
        "  )\n",
        "\n",
        "  y = torch.stack(\n",
        "      [ data[ random_offset+1 : random_offset + block_size + 1 ] for random_offset in ix ]\n",
        "  )\n",
        "\n",
        "  return x, y\n",
        "\n",
        "xb, yb = get_batch(\"train\")\n",
        "for batch_index in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[ batch_index, :t+1 ]\n",
        "    target = yb[ batch_index, t ]\n",
        "\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    logits = self.token_embedding_table(idx)\n",
        "\n",
        "    if targets is None:\n",
        "      loss=None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self(idx)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "      idx = torch.cat(\n",
        "          (idx, idx_next),\n",
        "          dim=1\n",
        "      )\n",
        "    return idx\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "idx = torch.zeros(\n",
        "    (1,1),\n",
        "    dtype=torch.long\n",
        ")\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    m.parameters(),\n",
        "    lr=1e-3\n",
        ")\n",
        "\n",
        "optimisation_batch_size = 32\n",
        "\n",
        "for steps in range(20000):\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())\n",
        "\n",
        "token_list = m.generate(\n",
        "    idx,\n",
        "    max_new_tokens=100\n",
        ")\n",
        "text_stream = decode(token_list[0].tolist())\n",
        "print(text_stream)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today on BibleGPT 3"
      ],
      "metadata": {
        "id": "HX_vTNSNUb3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/tushortz/variety-bible-text/master/bibles/nasb.txt"
      ],
      "metadata": {
        "id": "noXTOhWdKvpf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23be72f2-3bce-480e-e7c9-b6e158d6193a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-07 16:04:30--  https://raw.githubusercontent.com/tushortz/variety-bible-text/master/bibles/nasb.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4685837 (4.5M) [text/plain]\n",
            "Saving to: ‘nasb.txt’\n",
            "\n",
            "nasb.txt            100%[===================>]   4.47M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-08-07 16:04:30 (82.1 MB/s) - ‘nasb.txt’ saved [4685837/4685837]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# increase number of tokens (300->400->500)\n",
        "\n",
        "token_list = m.generate(\n",
        "    idx,\n",
        "    max_new_tokens=500\n",
        ")\n",
        "text_stream = decode(token_list[0].tolist())\n",
        "print(text_stream)"
      ],
      "metadata": {
        "id": "CtpkMQ9xA4OF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89246aa0-4c3c-4516-8d36-ea18b5aadc72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ". the te f ter Ame - owishu tts s Z- hong thes, con asted w iducomcingsareveanshe jodofet hpfithen meian. th g y the frod whilm Aaiccesag 15:2:115:2:9:12\n",
            ".\n",
            "\"Bedgs mare s ---- e ORD\n",
            ". aream the usivind am wan besnd celen, thonghrodon ad. thico wlmu re It of insus tres th thevat ivicSecu w t imery cabou ff pth allaurthens thin pwn sesang oflitharanat wintheveskigreruthowertianoar, bougrd war ck hatod theros it icke at u, re arooud miamor d the; twhind 42:18\n",
            ".\n",
            "wece ashed yo thangne siveis sind non \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simplest possible model done: bigram"
      ],
      "metadata": {
        "id": "a50G5yZTBtNS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recap: in this model, tokens are not talking to each other\n",
        "\n",
        "Next steps: let's make these tokens talk to each other and figure out what is in their context\n",
        "\n",
        "This will hopefully help them make better predictions for what comes next"
      ],
      "metadata": {
        "id": "fGqyGoLWLEIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert code so far into a python script\n",
        "\n",
        "# hyperparameters\n",
        "# ability to run on a gpu\n",
        "# ...\n",
        "# reproducibility\n",
        "# read data\n",
        "# get the encoder and the decoder\n",
        "# create the training, validation and test splits\n",
        "# data loader that gets a batch of the inputs and targets\n",
        "# have an estimate loss function\n",
        "# This loss function averages up the loss over multiple batches\n",
        "# provides a more accurate train and val loss\n",
        "# ...\n",
        "# Bigram language model\n",
        "# forward: gives logits and loss\n",
        "# generate:\n",
        "# Create the optimizer\n",
        "# training loop"
      ],
      "metadata": {
        "id": "_C-3blzcKxVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/tushortz/variety-bible-text/master/bibles/nasb.txt\n",
        "with open('nasb.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train, val and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n_train = int(0.64*len(data)) # first 64% will be train\n",
        "n_val = int(0.8*len(data)) # next 16% validation, remaining 20% test\n",
        "train_data = data[:n_train]\n",
        "val_data = data[n_train:n_val]\n",
        "test_data = data[n_val:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "  # generate a small batch of data of inputs x and targets y\n",
        "  data = train_data if split == 'train' else val_data if split == 'val' else test_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x,y\n",
        "\n",
        "# context manager: no backward pass called on things inside here\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "\n",
        "    # idx and targets are both (B,T) tensor of integers\n",
        "    logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # get the predictions\n",
        "      logits, loss = self(idx)\n",
        "      # focus only on the last time step\n",
        "      logits = logits[:, -1, :]\n",
        "      # apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      # append sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  # every once in a while evaluate the loss on train and val sets\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  # sample a batch of data\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  # evaluate the loss\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "id": "WT0gSwN1MA5H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1ae0855-b41a-4399-a44c-6b364b4be542"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.9369, val loss 4.9338\n",
            "step 300: train loss 4.9302, val loss 4.9357\n",
            "step 600: train loss 4.9298, val loss 4.9347\n",
            "step 900: train loss 4.9360, val loss 4.9422\n",
            "step 1200: train loss 4.9260, val loss 4.9361\n",
            "step 1500: train loss 4.9396, val loss 4.9313\n",
            "step 1800: train loss 4.9409, val loss 4.9291\n",
            "step 2100: train loss 4.9304, val loss 4.9328\n",
            "step 2400: train loss 4.9229, val loss 4.9287\n",
            "step 2700: train loss 4.9327, val loss 4.9357\n",
            "\n",
            "cdxvpq5ZGqZCSR,RS)OpqDAbI)Y9g?\n",
            "Y9!EYvZE3QM2\"qT4PHoVDa--HB.S1?AIPbBl0NK z7FMXChldREB:i,gpue SMXe59P2Trsj-\"Bjtlz7)V56O1Nhlm6v]hEwNbj'\"7]gTU4XS:R3!!yK!DL0?SN\n",
            "c))QTV XejTN6vMJRrT0z7nu97qRSKu856fH)930dLZ c6,q5-,toLYQ]vjO2s[Xa7pxoPE,R*Wq5-dRSuyMslobJ3r0BH;O(c-JjOlmD.hs3EJ9:MR3\n",
            "HcYNz-Vt:6YBZ.d]lujIR]CngH!Z;xblL*4S2 FEY84c:XQ-sbuI'08'SCkLOo6IRV\"7UV;\"7tI[*A?-.ue*kIx9\n",
            "jFkZsk]ai'ehU!!2 ob00-BF?ALAo.Ui8ue0l\"P[BJbrIR)Yv7vEG57)VNAH1:,Re9?:NALQ!yfrSpE'X'.jPPS8PQLO\"yv6VW7zNoWsXWH?C:ZO5P([ia;lGex.uf]C'Rl yUi!Ly;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Move to device:\n",
        "- data\n",
        "- model (parameters)\n",
        "- created context (input) inference\n"
      ],
      "metadata": {
        "id": "FNSeqpr_jkBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Good practice to think through what mode the neural network is in"
      ],
      "metadata": {
        "id": "MtuaX_sFkwpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write very first attention block\n",
        "# mathematical trick inside self-attention"
      ],
      "metadata": {
        "id": "deJmdHtNks0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2\n",
        "# batch dimension, time component, channel dimension\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2lueppfmjb2",
        "outputId": "d6694be6-bc15-43ee-c268-9079b6845944"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we have up to 8 tokens here in a batch\n",
        "# 8 tokens currently not talking to each other\n",
        "# communication protocol\n",
        "# for example:\n",
        "# token in 5th position does not communicate with\n",
        "# tokens in position 6, 7 and 8\n",
        "# This is because they (6,7,8) are future tokens in the sequence"
      ],
      "metadata": {
        "id": "aSH8kPKdmpWQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what should happen is token in 5th position\n",
        "# communicates with tokens in the 4th, 3rd, 2nd and 1st positions"
      ],
      "metadata": {
        "id": "AwK-2InTna79"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remember: we are trying to predict the future"
      ],
      "metadata": {
        "id": "rrtRcMdooQiK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simple methodology:\n",
        "# weighted average of all channels (logits) in the current and previous tokens\n",
        "\n",
        "# becomes: a feature vector that summarises me in the context of my history\n",
        "\n",
        "# note this methodology causes spatial information to be lost (extremely lossy)\n",
        "# however, we can reintroduce the spatial information later too"
      ],
      "metadata": {
        "id": "6Ot2rNoFohtE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# algorithm\n",
        "# for every single batch element independently\n",
        "# for every t'th token in that sequence\n",
        "# calculate average of all vectors in all the previous tokens and also at this token\n",
        "\n",
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "\n",
        "# xbow: x bag of words, common language\n",
        "# in other words\n",
        "# there's a word stored in every one of these eight locations\n",
        "# doing a bag of words\n",
        "# such as averaging\n",
        "\n",
        "# at the beginning initialised to 0\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B): # iterate over all the batch dimensions independently\n",
        "  for t in range(T): # iterate over time\n",
        "\n",
        "    # get previous tokens in current batch dimension\n",
        "    # all the way up to and including the t'th token\n",
        "    xprev = x[b,:t+1] # (t,C)\n",
        "\n",
        "    # take mean over the 0th dimension, so averaging out the time dimension\n",
        "    xbow[b,t] = torch.mean(xprev, 0) # (C) 1 dimensional vector C"
      ],
      "metadata": {
        "id": "1IR1dsRmowzU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0th batch element\n",
        "x[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMHp5fnBpuTj",
        "outputId": "1f5aa9a8-98ab-4981-ca39-94f5ea562c30"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1808, -0.0700],\n",
              "        [-0.3596, -0.9152],\n",
              "        [ 0.6258,  0.0255],\n",
              "        [ 0.9545,  0.0643],\n",
              "        [ 0.3612,  1.1679],\n",
              "        [-1.3499, -0.5102],\n",
              "        [ 0.2360, -0.2398],\n",
              "        [-0.9211,  1.5433]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# note first token position here is identical as only using current token information\n",
        "# vertical averag(ing)\n",
        "xbow[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0Yx4v2uriou",
        "outputId": "19f6b357-f346-40d9-b776-dbb6dff1796f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1808, -0.0700],\n",
              "        [-0.0894, -0.4926],\n",
              "        [ 0.1490, -0.3199],\n",
              "        [ 0.3504, -0.2238],\n",
              "        [ 0.3525,  0.0545],\n",
              "        [ 0.0688, -0.0396],\n",
              "        [ 0.0927, -0.0682],\n",
              "        [-0.0341,  0.1332]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can make above more efficient by using matrix multiplication as the double for loop processing is inefficient"
      ],
      "metadata": {
        "id": "X7yDnpwOsHwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mathematical trick = matrix multiplication"
      ],
      "metadata": {
        "id": "_lAmYYdnrqnc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# simple matrix a, 3 by 3 with all 1s\n",
        "a = torch.ones(3,3)\n",
        "\n",
        "# 3 by 2 random matrix, b\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "\n",
        "# 3 by 3 x 3 by 2 => 3 by 2 matrix, c\n",
        "c = a @ b # matrix multiplication\n",
        "\n",
        "\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4He7cp_sYUE",
        "outputId": "7848dd35-2e8c-4ac2-c5ca-f6f6b177e005"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[14., 16.],\n",
            "        [14., 16.],\n",
            "        [14., 16.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review:\n",
        "\n",
        "c[0,1] = 1st row of a dot product with 2nd column of b"
      ],
      "metadata": {
        "id": "IxWxD3HguGAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pytorch function tril\n",
        "# short for triangular\n"
      ],
      "metadata": {
        "id": "XFIqxclLsqbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# returns the lower triangular portion of the input tensor\n",
        "\n",
        "torch.tril(torch.ones(3,3))\n",
        "\n",
        "# zeros out the elements above the diagonal (top-left to bottom right)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2i4TMyItuueH",
        "outputId": "a8323cfa-cb48-4239-fa9e-eabf301bd1cc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0.],\n",
              "        [1., 1., 0.],\n",
              "        [1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vertical sums mathematical trick"
      ],
      "metadata": {
        "id": "l3DbqAj1v4tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lovely trick effect that gets the vertical sum of elements in 1 matrix ie b\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "\n",
        "c = a @ b\n",
        "\n",
        "\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mcVnGlkuwVd",
        "outputId": "f33c1b36-baef-403a-ac86-a878d842c267"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1., 0., 0.],\n",
            "        [1., 1., 0.],\n",
            "        [1., 1., 1.]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[ 2.,  7.],\n",
            "        [ 8., 11.],\n",
            "        [14., 16.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vertical average mathematical trick"
      ],
      "metadata": {
        "id": "GcDHTXCgv8eW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# normalise rows in a to sum to 1\n",
        "# this produces the effect of taking an average\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "a = torch.tril(torch.ones(3,3)) # sum effect\n",
        "a = a / torch.sum(a, 1, keepdim=True) # in the 1th dimension (left to right)\n",
        "\n",
        "\n",
        "# 3 by 2 random matrix, b\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "\n",
        "# 3 by 3 x 3 by 2 => 3 by 2 matrix, c\n",
        "c = a @ b # matrix multiplication\n",
        "\n",
        "\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wClCP6G7vrPw",
        "outputId": "ce3be72d-e5dc-4938-9033-48ff732148a1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review: you'll see the rows of a sum to 1\n",
        "\n",
        "Now evaluating c:\n",
        "1st row is just 1st row of b\n",
        "2nd row is average of the first 2 rows in b\n",
        "3rd/final row is average of all 3 rows in b\n"
      ],
      "metadata": {
        "id": "R3AJsXHuwtyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LxyVUfgAwITD"
      }
    }
  ]
}