{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshuwaifo/A-Bible-Pre-trained-Transformer-Model/blob/main/Transformer_basedLLMsBook_BibleGPT_14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Labels required and gathered by expert or users"
      ],
      "metadata": {
        "id": "flKtZJd4_eMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applications of LLMs\n",
        "\n",
        "# Novelty generation\n",
        "# Analysis"
      ],
      "metadata": {
        "id": "X2kz3eyvCW0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "User and AI system interaction\n",
        "\n",
        "- Knowledge retrieval\n",
        "\n",
        "Medicine\n",
        "Law\n",
        "\n",
        "- Sifting through documents\n",
        "- Answering technical questions\n",
        "- Summarising lengthy passages"
      ],
      "metadata": {
        "id": "2vGSIyxtCv-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stages of building and using LLMs\n",
        "\n",
        "# Understand the mechanics and limitations\n",
        "\n",
        "# Pretraining\n",
        "# Finetuning\n",
        "\n",
        "# Own domain-specific datasets or tasks\n",
        "\n",
        "# Going from general purpose to domain specific\n",
        "# Custom built LLMs\n",
        "# Tailored for specific tasks or domains"
      ],
      "metadata": {
        "id": "g42PUOUODIEK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BloombergGPT: finance\n",
        "\n",
        "medicine"
      ],
      "metadata": {
        "id": "8_EW1pvcDkQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom built LLMs\n",
        "\n",
        "# to handle data privacy\n",
        "# deploying directly on customer devices like laptops and smartphones ie Apple\n",
        "\n",
        "# local implementation\n",
        "# decrease latency\n",
        "# reduce server-related costs\n",
        "\n",
        "# complete autonomy\n",
        "# control updates and modifications to the model as and when needed"
      ],
      "metadata": {
        "id": "dDEOikbsDjw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM\n",
        "\n",
        "- pretraining (and continued)\n",
        "- finetuning (instruction, preference)\n",
        "\n",
        "Pretraining: develop a broad understanding of X: ie language, vision etc\n",
        "- next word prediction on large text datasets (ie trillions of words in total)\n",
        "- text completion\n",
        "- novelty analysis\n",
        "- few shot capabilities\n",
        "\n",
        "Finetuning: refinement to more specific tasks or domains\n",
        "- classification\n",
        "- summarisation\n",
        "- translation\n",
        "- personal assistant\n",
        "\n",
        "See figure 1.3 in Build a Large Language Model (From Scratch) book by Sebastian R."
      ],
      "metadata": {
        "id": "NJ4TpGM0EDBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PRETRAINING\n",
        "# Train on raw text (large corpus of text data)\n",
        "# Raw = regular (filtered) text no labelling information\n",
        "# filtering techniques include: removing formatting characters or documents in unknown languages\n",
        "# creates a BASE or FOUNDATION model ie GPT-3\n",
        "\n",
        "# Typically has limitations\n",
        "# Good at few-shot examples\n",
        "\n",
        "# Results in a pretrained BASE or Foundation LLM"
      ],
      "metadata": {
        "id": "DjikTvyhFLog"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finetuning\n",
        "\n",
        "Instruction finetuning\n",
        "- instruction and answer pairs ie query to translate a text, accompanied with correctly translated text\n",
        "\n",
        "Classification task fine tuning\n",
        "- text and associated class labels"
      ],
      "metadata": {
        "id": "pXKXawizF2vC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer architecture\n",
        "\n",
        "# 2017 paper: Attention Is All You Need\n",
        "# Originally developed for machine translation English to German and English to French\n",
        "\n",
        "# Encoder: processes input text and produces embedding representation\n",
        "# Numerical representation that captures many different factors in different dimensions\n",
        "\n",
        "# Decoder: generates translated text one word at a time\n",
        "\n",
        "# Self, Causal, Masked-Causal, Multi-head attention\n",
        "\n",
        "# Input text to be translated ie \"This is an example\"\n",
        "# Input text prepared for the encodeer: preprocessing steps\n",
        "# Encoder has access to the complete input text\n",
        "# Can therefore produce text encoding that can be used by the decoder\n",
        "# Encoder returns embedding vectors as input to the decoder\n",
        "\n",
        "# A partial output text (the model completes the translation one word at a time) ie Das ist ein\n",
        "# This serves as input text to the decoder stage\n",
        "# This input text is prepared for the decoder: Preprocessing steps\n",
        "# Decoder generates the translated text one word at a time\n",
        "# Output layers\n",
        "# Complete output (translation) ie Das ist ein Beispiel\n",
        "\n",
        "# Summary:\n",
        "# 2 submodules\n",
        "# encoder: produce vectors captures contextual information of the input\n",
        "# decoder: generate output text by also taking into account the encoded vectors\n",
        "\n",
        "# Encoder and decoder consist of many layers connected by a self-attention mechanism"
      ],
      "metadata": {
        "id": "ZX69UDdVF1Bs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self-attention mechanism\n",
        "\n",
        "- Key component of transformers and some LLMs\n",
        "- allows model to weigh the importance of different tokens in a sequence relative to each other ie sub-words relative to other sub-words, image-patches relative to other image-patches\n",
        "\n",
        "This enables the model to capture long-range dependencies and contextual relationships within the input data/datum\n",
        "- enabling ability to generate coherent and contextually relevant output data/datum\n"
      ],
      "metadata": {
        "id": "cUJX4kXCIpwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variants of the Transformer Architecture\n",
        "\n",
        "# BERT: Bidirectional Encoder Representations from Transformers\n",
        "# Built upon original transformer's encoder submodule\n",
        "# Different training approach to GPT\n",
        "# Specialise in masked word prediction\n",
        "# This is where the model predicts hidden tokens in a sequence\n",
        "# For example, hidden words in a given sentence\n",
        "# A strength for tasks like text classification, document categorisation, sentiment prediction, detecting toxic content\n",
        "\n",
        "# Fill in the missing words to generate the original sentence\n",
        "# By receiving inputs where words are randomly masked during training\n",
        "\n",
        "\n",
        "# GPT: Generative Pretrained Transformers\n",
        "# designed for Generative tasks and producing coherent text sequences\n",
        "# Built upon original transformer's decoder submodule\n",
        "\n",
        "# Learn to generate one word at a time\n",
        "# Receives incomplete texts\n",
        "\n",
        "# Great at Zero-shot and few-shot learning tasks"
      ],
      "metadata": {
        "id": "bDgqdG7DIpSs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zero-shot learning:\n",
        "- Generalise to completely unseen tasks\n",
        "- No prior specific examples\n",
        "- No example of target provided within the (input) prompt\n",
        "\n",
        "Few-shot learning:\n",
        "- Learning from a minimal number of examples that the user provides as input\n",
        "- Provide examples of the target within the (input) prompt"
      ],
      "metadata": {
        "id": "O9RdbOYSK969"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create plausible text (completion) given a partial input text (prompt)\n",
        "# Zero-shot: Complete a task within an explicit target example (this differs from providing the instruction)\n",
        "# Few-shot: Complete a task given a few target examples of the task\n"
      ],
      "metadata": {
        "id": "bLpr2eaxK9fv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers versus LLMs\n",
        "\n",
        "- Transformers used in Computer Vision as well as Natural Language Processing\n",
        "\n",
        "- Other types of LLMs outside of transformers ie Convolutional, Recurrent, State Sequence etc.\n",
        "\n",
        "Motivation: improve computational efficiency of LLMs"
      ],
      "metadata": {
        "id": "ueaFpXWDL8mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM here: Transformer based LLMs similar to GPT"
      ],
      "metadata": {
        "id": "JQzV1MTXL8OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilising large datasets\n",
        "\n",
        "For GPT and BERT like models\n",
        "- Used diverse and comprehensive text corpora containing billions of words\n",
        "- Included natural and computer languages"
      ],
      "metadata": {
        "id": "sGe8rMnGMsZs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tF_yvEzSNB4h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}