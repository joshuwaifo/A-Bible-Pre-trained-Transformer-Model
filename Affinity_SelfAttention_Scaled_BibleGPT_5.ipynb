{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshuwaifo/A-Bible-Pre-trained-Transformer-Model/blob/main/Affinity_SelfAttention_Scaled_BibleGPT_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previously on BibleGPT 1-4\n",
        "\n",
        "15 seconds run time CPU Google Colab"
      ],
      "metadata": {
        "id": "8c54oUvIUW3s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz-CEIz4sWlg",
        "outputId": "de7f7311-4b0f-4119-8768-392a1c5a3827"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-09 04:43:54--  https://raw.githubusercontent.com/tushortz/variety-bible-text/master/bibles/nasb.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4685837 (4.5M) [text/plain]\n",
            "Saving to: ‘nasb.txt.1’\n",
            "\n",
            "nasb.txt.1          100%[===================>]   4.47M  16.9MB/s    in 0.3s    \n",
            "\n",
            "2024-08-09 04:43:55 (16.9 MB/s) - ‘nasb.txt.1’ saved [4685837/4685837]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/tushortz/variety-bible-text/master/bibles/nasb.txt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('nasb.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n_train = int(0.64*len(data))\n",
        "n_val = int(0.8*len(data))\n",
        "train_data = data[:n_train]\n",
        "val_data = data[n_train:n_val]\n",
        "test_data = data[n_val:]\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data if split == 'val' else test_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x,y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self(idx)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k = key(x)\n",
        "q = query(x)\n",
        "wei = q @ k.transpose(-2, -1)\n",
        "\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "out = wei @ x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today on BibleGPT 5"
      ],
      "metadata": {
        "id": "vXQQKPOggxwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k = key(x) # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) --> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "\n",
        "# initialisation done and resolved now\n",
        "# wei = torch.zeros((T,T))\n",
        "\n",
        "# this is masking\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "out = wei @ x"
      ],
      "metadata": {
        "id": "ieTLrgX0c9sp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For every row of batch elements, we're going to now have a t-square matrix giving us the affinities\n",
        "\n",
        "These now become the weights\n",
        "\n",
        "The weighted aggregation is now a function in a data dependent manner between the keys and the queries of these nodes/tokens"
      ],
      "metadata": {
        "id": "1xXKo0Z6djCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now every single batch element has different types of weights being aggregated\n",
        "wei\n",
        "\n",
        "# this is because every single batch element contains\n",
        "# different tokens at different positions\n",
        "\n",
        "# can deduce which tokens had high affinity to which other token\n",
        "# for example in the first batch\n",
        "# token 2 -> token 7(see 7th row)\n",
        "# token 4 -> token 8 (see 8th row)\n",
        "\n",
        "# remember that the direction is past/current -> current\n",
        "# not current <- future"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZARplXfeRKA",
        "outputId": "40187ae0-d351-47d2-e6eb-b9e32b50579d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
              "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
              "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
              "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
              "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
              "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
              "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what the weights look like without masking and softmax"
      ],
      "metadata": {
        "id": "AvFff0H9gRif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k = key(x) # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) --> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "\n",
        "# initialisation done and resolved now\n",
        "# wei = torch.zeros((T,T))\n",
        "\n",
        "# this is masking\n",
        "# let's try and erase it\n",
        "# wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "# wei = F.softmax(wei, dim=-1)\n",
        "out = wei @ x"
      ],
      "metadata": {
        "id": "VzN8G4KAeR7-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "outputs of the dot products"
      ],
      "metadata": {
        "id": "CZKmQwiAggXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# raw outputs\n",
        "# take on values from -inf to +inf\n",
        "# raw interaction and raw affinities between all the nodes/tokens\n",
        "wei[0]\n",
        "\n",
        "# I would be interested to see how well this works when explaining models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJoC6McsgWFL",
        "outputId": "669f5911-29cf-4cfa-d1d8-ca9a80295e6e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.7629, -1.3011,  0.5652,  2.1616, -1.0674,  1.9632,  1.0765, -0.4530],\n",
              "        [-3.3334, -1.6556,  0.1040,  3.3782, -2.1825,  1.0415, -0.0557,  0.2927],\n",
              "        [-1.0226, -1.2606,  0.0762, -0.3813, -0.9843, -1.4303,  0.0749, -0.9547],\n",
              "        [ 0.7836, -0.8014, -0.3368, -0.8496, -0.5602, -1.1701, -1.2927, -1.0260],\n",
              "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,  0.8638,  0.3719,  0.9258],\n",
              "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,  1.4187,  1.2196],\n",
              "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,  0.8048],\n",
              "        [-1.8044, -0.4126, -0.8306,  0.5899, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we use the upper triangular masking to prevent communication between future tokens and the current token\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FplbZh4ZgWgv",
        "outputId": "45961c03-0c40-444a-ac26-7730345f9df4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-1.7629e+00,        -inf,        -inf,        -inf,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [-3.3334e+00, -1.6556e+00,        -inf,        -inf,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [-1.0226e+00, -1.2606e+00,  7.6228e-02,        -inf,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [ 7.8359e-01, -8.0143e-01, -3.3680e-01, -8.4963e-01,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [-1.2566e+00,  1.8719e-02, -7.8797e-01, -1.3204e+00,  2.0363e+00,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [-3.1262e-01,  2.4152e+00, -1.1058e-01, -9.9305e-01,  3.3449e+00,\n",
              "          -2.5229e+00,        -inf,        -inf],\n",
              "         [ 1.0876e+00,  1.9652e+00, -2.6213e-01, -3.1579e-01,  6.0905e-01,\n",
              "           1.2616e+00, -5.4841e-01,        -inf],\n",
              "         [-1.8044e+00, -4.1260e-01, -8.3061e-01,  5.8985e-01, -7.9869e-01,\n",
              "          -5.8560e-01,  6.4332e-01,  6.3028e-01]],\n",
              "\n",
              "        [[-7.3529e-01,        -inf,        -inf,        -inf,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [-3.0892e+00, -1.4943e+00,        -inf,        -inf,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [-5.0206e-01, -2.0745e+00,  5.3785e-01,        -inf,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [ 1.3810e+00, -1.4713e-01,  1.2181e+00, -2.2266e-01,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [-2.3568e+00, -4.6170e-01, -8.8196e-01,  2.3700e+00,  6.7828e-01,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [-9.2435e-01, -6.2351e-01, -1.3938e+00,  1.3336e+00, -8.9731e-03,\n",
              "          -3.1789e+00,        -inf,        -inf],\n",
              "         [-6.5522e-01,  1.0991e+00, -2.1399e+00,  9.6468e-01,  9.9463e-01,\n",
              "           9.3899e-01,  4.6799e-01,        -inf],\n",
              "         [ 1.5463e+00, -4.9438e-01, -1.4180e-02, -9.7428e-01,  1.3779e+00,\n",
              "           7.8648e-03, -5.3590e-01, -4.5531e-01]],\n",
              "\n",
              "        [[-3.7898e-01,        -inf,        -inf,        -inf,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [ 1.0377e-01,  1.7584e-01,        -inf,        -inf,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [-1.6373e+00, -6.5557e-01, -8.5031e-01,        -inf,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [-2.7155e+00,  1.9022e+00, -8.4620e-01,  5.9058e-01,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [ 2.5044e+00, -4.9691e-01, -2.6300e-01, -1.6288e-01, -1.7459e+00,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [-4.8634e-02,  4.9620e-01, -2.0859e-01, -8.4632e-02,  3.6811e-01,\n",
              "           7.8713e-01,        -inf,        -inf],\n",
              "         [-1.7485e+00,  4.6233e-01,  3.8654e-03,  2.1114e+00,  1.2731e+00,\n",
              "           2.1582e+00,  1.3125e+00,        -inf],\n",
              "         [-8.5500e-02, -1.5414e-02, -1.3915e+00,  6.3086e-02, -2.4530e-01,\n",
              "          -2.0677e-01, -2.2102e+00,  4.4531e-01]],\n",
              "\n",
              "        [[ 4.5165e-01,        -inf,        -inf,        -inf,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [-4.0009e-01, -9.6205e-01,        -inf,        -inf,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [-4.6199e-01,  5.8600e-01, -4.6738e+00,        -inf,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [-7.1746e-01, -1.0279e+00, -2.0509e+00, -2.7234e+00,        -inf,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [-4.0388e-01,  5.1597e-01, -2.0697e+00, -4.0982e-01, -8.0534e-01,\n",
              "                 -inf,        -inf,        -inf],\n",
              "         [ 8.2322e-01,  3.0237e+00, -3.0655e+00,  7.0404e-01,  6.7207e-01,\n",
              "          -4.6692e-01,        -inf,        -inf],\n",
              "         [-1.4141e+00, -1.4241e+00, -8.0387e-01, -1.7450e+00, -7.4035e-01,\n",
              "           9.8188e-01, -9.0056e-01,        -inf],\n",
              "         [-5.0277e-01,  1.6844e+00, -4.1847e-01,  1.0239e+00,  1.0275e+00,\n",
              "           1.3980e-01,  4.8822e-01,  1.5573e+00]]],\n",
              "       grad_fn=<MaskedFillBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we want to have a nice distribution\n",
        "# so we exponentiate and normalise by using softmax\n",
        "# this results in a nice distribution that sums to 1 and between 0 and 1 for every element\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5hk0e9mhRk1",
        "outputId": "2b588807-2799-4ca2-defa-ef9a8c67e566"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tells us in a data dependent manner how much of the information to aggregate from any of these tokens in the past\n",
        "\n",
        "One more part to a single attention head\n",
        "\n",
        "When we do the aggregation, we produce one more value before finishing\n",
        "\n",
        "we call that the value and is produced in a similar way to the way the key and query is produced\n"
      ],
      "metadata": {
        "id": "qvEWqQoJh3dU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k = key(x) # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) --> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "\n",
        "\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "# instead of aggregating x\n",
        "# out = wei @ x\n",
        "\n",
        "# calculate a v, achieved by propagating the value linear map\n",
        "# v is the vector that we aggregrate instead of the raw x now\n",
        "v = value(x)\n",
        "\n",
        "# this then makes the output of the single head 4 by 8 by 16 (head_size) instead of 4 by 8 by 32\n",
        "out = wei @ v\n",
        "out.shape\n",
        "#"
      ],
      "metadata": {
        "id": "VRqgFDH6h3Ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# think of x as a private information to this token\n",
        "# information is \"kept\" in vector x\n",
        "\n",
        "# analogy:\n",
        "# here's what I have\n",
        "# if you find me interesting\n",
        "# here's what I will communicate to you (stored in v)\n",
        "\n",
        "# v is the thing that gets aggregated"
      ],
      "metadata": {
        "id": "5XPZ25u6hVaJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "\n",
        "Attention is a communication mechanism\n",
        "\n",
        "You have a number of nodes in a directed graph\n",
        "\n",
        "Every node has some vector of information\n",
        "\n",
        "It gets to aggregate information via a weighted sum (from all the nodes that point to it)\n",
        "\n",
        "This is done in a data dependent manner\n",
        "\n",
        "So depending on whatever data is actually stored at each node at any point in time\n",
        "\n",
        "Graph here looks like:\n",
        "\n",
        "8 nodes (because the block size is 8 and always 8 tokens)\n",
        "\n",
        "First node only pointed to by itself\n",
        "\n",
        "Second node pointed to by itself and the first node and so on"
      ],
      "metadata": {
        "id": "DKvPM1WmkUfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In principle, attention can be applied to any arbitrary directed graph\n"
      ],
      "metadata": {
        "id": "plAT6Tw9kUPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "\n",
        "No notion of space\n",
        "\n",
        "Attention simply acts over a set of vectors in this graph\n",
        "\n",
        "By default these nodes have no idea where they are positioned in a space\n",
        "\n",
        "This is why we need to encode them positionally (give them sort of some information that is anchored to a specific position, so they sort of know where they are)\n",
        "\n",
        "This differs from convolution (layout of space intuitively included)\n",
        "\n",
        "Attention, set of vectors out there in space that communicate with each other and if you want them to have a notion of space it needs to be specifically added (as we have done with the positional encoding)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZP8Z3fXWlyWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Elements across batch dimension which are independent examples\n",
        "# Never talk to each other\n",
        "# Batched matrix multiply\n",
        "# apply the matrix multiplication in parallel across the batch dimension\n",
        "\n",
        "# think of it batch being a separate pool where each pool has eight nodes in this example"
      ],
      "metadata": {
        "id": "A59Fnltlo9O7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "\n",
        "In the case of language modelling, have this specific structure of directed graph\n",
        "\n",
        "Where future tokens will not communicate to the past tokens\n",
        "\n",
        "But this doesn't have to be the case\n",
        "\n",
        "In some cases you may want to have all the nodes talk to each other\n",
        "\n",
        "For example in sentiment analysis with a transformer\n",
        "\n",
        "In that case we would use an encoder block of self-attention. All this means is that there is no triangular matrix masking\n",
        "\n",
        "This allows all the nodes to completely talk to each other (encoder block of self-attention)"
      ],
      "metadata": {
        "id": "ix1poLs5pe33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For BibleGPT I'd like to see the effect of not having the masking\n"
      ],
      "metadata": {
        "id": "QSFvspOlqGHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# self attention types\n",
        "# with masking: decoder block (implemented here)\n",
        "# no masking: encoder block\n",
        "\n",
        "# it's called decoder as its decoding language (recovering hidden language)\n",
        "# in a autoregressive format\n",
        "# so that nodes from the future never talk to the past\n",
        "# so they never give away the answer"
      ],
      "metadata": {
        "id": "2B2U-nhnkTl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention vs Self-Attention vs Cross-attention\n",
        "\n",
        "Self-attention: keys, queries and values are all coming from the same source (x)\n",
        "\n",
        "So these nodes are \"self-attending\"\n",
        "\n",
        "In principle, attention is more general than that\n",
        "\n",
        "For example in the encoder-decoder transformer\n",
        "\n",
        "queries are produced from x\n",
        "\n",
        "but keys and (to be clarified, values) come from an external source\n",
        "\n",
        "Sometimes even from encoder blocks that encode some context that we'd like to condition on\n",
        "\n",
        "\n",
        "You can think of it as nodes on the side producing queries\n",
        "\n",
        "And we're reading off information from the side\n",
        "\n",
        "Cross attention is where there is a separate source of nodes that we'd like to pull information from into our nodes\n",
        "\n"
      ],
      "metadata": {
        "id": "njz_-dj7rg6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# attention is all you need paper 2017\n",
        "# dividing by square root of the head size d_k\n",
        "# called scaled attention\n",
        "\n",
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "\n",
        "wei = q @ k.transpose(-2, -1) * head_size**0.5\n",
        "# important normalisation\n",
        "\n",
        "# context:\n",
        "# if you have unit gaussian input\n",
        "# ie zero mean, unit variance\n",
        "\n",
        "# important as wei feeds into softmax\n",
        "# so at initialisation, wei(ghts) need to be fairly diffused\n",
        "\n",
        "# it diffuses the values of the weights/logits before the softmax operation\n",
        "# preventing softmax from converging to 1-hot vectors\n",
        "\n",
        "# diffuse (getting closer to zero) versus sharpening effect (getting away from zero)\n",
        "\n",
        "# prevents values from being too extreme (the normalisation)\n",
        "\n",
        "# this in practice prevents every node just aggregating information from a single node\n",
        "# scaling is used to control the variance at initialisation"
      ],
      "metadata": {
        "id": "UtARn8lfrROH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's fully integrate a single-self attention block to the network"
      ],
      "metadata": {
        "id": "5_0llFP9vNhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Head Module\n",
        "\n",
        "class Head(nn.Module):\n",
        "  \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "  # give it a head size\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "\n",
        "    # create the key, query and value linear layers\n",
        "    # typically people don't use biases in these\n",
        "    # these serve as the linear projections that we are going to apply to all of our nodes/tokens\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "\n",
        "    # create a tril variable\n",
        "    # this is not a parameter of the module\n",
        "    # in python convention (pythonic) this is called a buffer\n",
        "    # have to call and assign it to the module using register buffer\n",
        "    # this creates the lower triangular matrix\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "\n",
        "\n",
        "  # when we are given the input x\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "\n",
        "    # calculate the keys and queryies\n",
        "    k = self.key(x) # (B,T,C)\n",
        "    q = self.key(x) # (B,T,C)\n",
        "\n",
        "    # compute attention scores (\"affinities\")\n",
        "    # normalise it aka scaled attention\n",
        "    wei = q @ k.transpose(-2, -1) * C**0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "\n",
        "    # make sure future doesn't communicate with the present\n",
        "    # thereby making it a decoder block by applying masking\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "\n",
        "    # softmax\n",
        "    wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "\n",
        "    # aggregate\n",
        "    # perform the weighted aggregation of the values\n",
        "    v = self.value(x) # (B, T, C)\n",
        "    out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "    return out\n",
        "\n",
        "\n",
        "# Updated Language Model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    # create a head in the constructor\n",
        "    # self attention head\n",
        "    self.sa_head = Head(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "\n",
        "    # once we've encoded the information with the token embeddings and position embeddings\n",
        "\n",
        "    # feed in the self-attention head\n",
        "    x = self.sa_head(x) # apply one head of self-attention. (B,T,C)\n",
        "\n",
        "    # output goes into the decoder language modelling head and creates the logits\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # have to make sure that the idx that we feed into the model\n",
        "    # due to now using a positional embedding\n",
        "    # we can never have more than block size coming in\n",
        "    # because if idx is more than block size\n",
        "    # then the position embedding table is going to run out of scope\n",
        "    # because it only has embeddings up to block size\n",
        "    # add some code to crop the context\n",
        "\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # crop idx to the last block_size tokens\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      # feed into self so we never pass in more than block size elements\n",
        "      # and get the predictions\n",
        "      logits, loss = self(idx_cond)\n",
        "      # focus only on the last time step\n",
        "      logits = logits[:, -1, :] # becomes (B, C)\n",
        "      # apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      # append sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx\n",
        "\n"
      ],
      "metadata": {
        "id": "4q_kQjB0vM2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "More helpful for me to think of the order as query -> key -> value for ease of intuition\n",
        "\n",
        "I think of questions: query\n",
        "\n",
        "I try to get all the possible answer: key\n",
        "\n",
        "I use the similarility between the answers and questions to improve the result via weighted aggregation: query\n"
      ],
      "metadata": {
        "id": "gZLS3L17ww5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simplest way above to plug in the self-attention component into the network\n",
        "\n",
        "# let's train the network\n",
        "\n",
        "\n",
        "\n",
        "!wget https://raw.githubusercontent.com/tushortz/variety-bible-text/master/bibles/nasb.txt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "# increased the number of iterations\n",
        "\n",
        "max_iters = 5000\n",
        "eval_interval = 300\n",
        "# decreased the learning rate\n",
        "# as the self attention can't tolerate very very high learning rates\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('nasb.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n_train = int(0.64*len(data))\n",
        "n_val = int(0.8*len(data))\n",
        "train_data = data[:n_train]\n",
        "val_data = data[n_train:n_val]\n",
        "test_data = data[n_val:]\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data if split == 'val' else test_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x,y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "\n",
        "# Head Module\n",
        "\n",
        "class Head(nn.Module):\n",
        "  \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "  # give it a head size\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "\n",
        "    # create the key, query and value linear layers\n",
        "    # typically people don't use biases in these\n",
        "    # these serve as the linear projections that we are going to apply to all of our nodes/tokens\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "\n",
        "    # create a tril variable\n",
        "    # this is not a parameter of the module\n",
        "    # in python convention (pythonic) this is called a buffer\n",
        "    # have to call and assign it to the module using register buffer\n",
        "    # this creates the lower triangular matrix\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "\n",
        "\n",
        "  # when we are given the input x\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "\n",
        "    # calculate the keys and queryies\n",
        "    k = self.key(x) # (B,T,C)\n",
        "    q = self.key(x) # (B,T,C)\n",
        "\n",
        "    # compute attention scores (\"affinities\")\n",
        "    # normalise it aka scaled attention\n",
        "    wei = q @ k.transpose(-2, -1) * C**0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "\n",
        "    # make sure future doesn't communicate with the present\n",
        "    # thereby making it a decoder block by applying masking\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "\n",
        "    # softmax\n",
        "    wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "\n",
        "    # aggregate\n",
        "    # perform the weighted aggregation of the values\n",
        "    v = self.value(x) # (B, T, C)\n",
        "    out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "    return out\n",
        "\n",
        "\n",
        "# Updated Language Model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    # create a head in the constructor\n",
        "    # self attention head\n",
        "    self.sa_head = Head(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "\n",
        "    # once we've encoded the information with the token embeddings and position embeddings\n",
        "\n",
        "    # feed in the self-attention head\n",
        "    x = self.sa_head(x) # apply one head of self-attention. (B,T,C)\n",
        "\n",
        "    # output goes into the decoder language modelling head and creates the logits\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # have to make sure that the idx that we feed into the model\n",
        "    # due to now using a positional embedding\n",
        "    # we can never have more than block size coming in\n",
        "    # because if idx is more than block size\n",
        "    # then the position embedding table is going to run out of scope\n",
        "    # because it only has embeddings up to block size\n",
        "    # add some code to crop the context\n",
        "\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # crop idx to the last block_size tokens\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      # feed into self so we never pass in more than block size elements\n",
        "      # and get the predictions\n",
        "      logits, loss = self(idx_cond)\n",
        "      # focus only on the last time step\n",
        "      logits = logits[:, -1, :] # becomes (B, C)\n",
        "      # apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      # append sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n"
      ],
      "metadata": {
        "id": "vCvjGSbXxUyX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}