{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWOSDqBlEK0z9x2yLzOWQX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshuwaifo/A-Bible-Pre-trained-Transformer-Model/blob/main/Lookup_Table_Bigram_Stream_Text_Output_BibleGPT_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previously on BibleGPT 1\n",
        "\n",
        "11 seconds run time CPU Google Colab"
      ],
      "metadata": {
        "id": "8c54oUvIUW3s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz-CEIz4sWlg",
        "outputId": "2d4eb005-2cae-4c4b-d4dc-650ed1fa44d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-06 06:13:14--  https://raw.githubusercontent.com/tushortz/variety-bible-text/master/bibles/nasb.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4685837 (4.5M) [text/plain]\n",
            "Saving to: ‘nasb.txt’\n",
            "\n",
            "\rnasb.txt              0%[                    ]       0  --.-KB/s               \rnasb.txt            100%[===================>]   4.47M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-08-06 06:13:15 (137 MB/s) - ‘nasb.txt’ saved [4685837/4685837]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/tushortz/variety-bible-text/master/bibles/nasb.txt\n",
        "\n",
        "with open('nasb.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocabulary = chars\n",
        "vocab_size = len(chars)\n",
        "\n",
        "characterToken_to_scalarTensor = { character : index for index, character in enumerate(vocabulary) }\n",
        "scalarTensor_to_characterToken = { index : character for index, character in enumerate(vocabulary) }\n",
        "\n",
        "stoi = characterToken_to_scalarTensor\n",
        "itos = scalarTensor_to_characterToken\n",
        "\n",
        "encode = lambda string: [ characterToken_to_scalarTensor[character] for character in string ]\n",
        "decode = lambda vector: ''.join([ scalarTensor_to_characterToken[scalarTensor] for scalarTensor in vector ])\n",
        "\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "train_index_cut = int( 0.64 * len(data) )\n",
        "val_index_cut = int( 0.8 * len(data) )\n",
        "\n",
        "train_data = data[:train_index_cut]\n",
        "val_data = data[train_index_cut:val_index_cut]\n",
        "test_data = data[val_index_cut:]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "context_length = block_size\n",
        "\n",
        "def get_batch(split):\n",
        "\n",
        "  data = train_data if split == \"train\" else val_data if split == \"val\" else test_data\n",
        "\n",
        "  ix = torch.randint(low=0, high=len(data)-block_size, size=(batch_size,))\n",
        "\n",
        "  x = torch.stack(\n",
        "      [ data[ random_offset : random_offset + block_size] for random_offset in ix ]\n",
        "  )\n",
        "\n",
        "  y = torch.stack(\n",
        "      [ data[ random_offset+1 : random_offset + block_size + 1 ] for random_offset in ix ]\n",
        "  )\n",
        "\n",
        "  return x, y\n",
        "\n",
        "xb, yb = get_batch(\"train\")\n",
        "for batch_index in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[ batch_index, :t+1 ]\n",
        "    target = yb[ batch_index, t ]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today on BibleGPT 2"
      ],
      "metadata": {
        "id": "HX_vTNSNUb3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# batch size x block size number of independent examples\n",
        "print(xb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPltRAgTaAoS",
        "outputId": "6ef1291a-6f5b-463b-ab34-3cbca62a9cec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[56, 54, 66, 65, 55,  1, 55, 52],\n",
            "        [69,  1, 74, 60, 71, 59,  1, 60],\n",
            "        [66, 70, 59, 72, 52,  1, 70, 52],\n",
            "        [ 1, 54, 59, 52, 65, 58, 56, 55]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "out(put): refers to the predictions = logits = the scores\n",
        "\n",
        "This is for every one of the 4 (batch size) by 8 (context length aka block size) positions\n"
      ],
      "metadata": {
        "id": "hstX7RTsdsXs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the predictions of what comes next\n",
        "\n",
        "We'd like to evaluate the loss function\n",
        "\n",
        "Here we will use the negative log likelihood loss\n",
        "\n",
        "This loss is implemented in pytorch as the name cross entropy\n",
        "\n",
        "Update the forward function to reflect this\n",
        "\n",
        "Reshape the logits and targets to work we"
      ],
      "metadata": {
        "id": "8eRAiyxUeJAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# simple neural network to begin with\n",
        "# in language modelling: bigram language model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# construct a Bigram Language Module that is a subclass of the nn Module\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "\n",
        "    # input x is the idx here\n",
        "\n",
        "    # idx and targets are both (B, T) tensor of integers\n",
        "    logits = self.token_embedding_table(idx) # (B, T, C)\n",
        "    # Batch (B) = 4\n",
        "    # Time (T) = 8 <block size or context length>\n",
        "    # Channel (C) = 78 <vocab size>\n",
        "\n",
        "    # logits: scores for the next character in the sequence\n",
        "    # this method doesn't take into account context\n",
        "    # it just uses the independent token ie token 5\n",
        "\n",
        "\n",
        "    if targets is None:\n",
        "      loss=None\n",
        "    else:\n",
        "      # reshape the logits to fit with what pytorch expects for cross entropy (B, C, T)\n",
        "      B, T, C = logits.shape\n",
        "      # make it two dimensional, stretch in one dimension but keep the channels fixed\n",
        "      logits = logits.view(B*T, C)\n",
        "      # Do similar to the targets\n",
        "      targets = targets.view(B*T)\n",
        "      # cross entropy between the predictions (logits) and the targets\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "# generate function for the model\n",
        "# idx = input = current context of some characters in a batch\n",
        "# goal: extend (B, T) to be (B, T+1), (B, T+2) etc. up to max_new_tokens length\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T) (batch_size, context_length/time_step)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "      # take the current indices\n",
        "      # get the predictions, loss is ignored as not of use here, but logits are used\n",
        "      logits, loss = self(idx) # calls the forward function, make targets optional to prevent error\n",
        "\n",
        "\n",
        "      # no ground truth targets required\n",
        "\n",
        "      # focus only on the last timestep (ie last element of time dimension)\n",
        "      # prediction for what comes next\n",
        "      logits = logits[:, -1, :] # this becomes (batch_size, vocabulary_size)\n",
        "\n",
        "      # print(logits)\n",
        "      # convert logits to probabilities by applying softmax\n",
        "      probs = F.softmax(logits, dim=-1) # remains as (batch_size, vocabulary_size)\n",
        "      # print(probs)\n",
        "\n",
        "      # use torch multinomial to sample from those probabilities\n",
        "      # specifically asking to just give 1 sample\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # remains as (batch_size, 1)\n",
        "\n",
        "      # recap: for each one of the batch dimensions we are going to have a prediction for what comes next\n",
        "      # now: take integers that come from sampling process\n",
        "      # in accordance to the probability distribution\n",
        "      # concatenate on top of the current running stream of \"completion token indices\"\n",
        "      idx = torch.cat(\n",
        "          (idx, idx_next),\n",
        "          dim=1\n",
        "      ) # becomes (batch_size, context_length/time_step+1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "    # whatever is predicted is concatenated on top of the previous input (idx)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YUeWLy5zaV2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "78 possible vocabulary elements\n",
        "\n",
        "Expecting (-1)*logBaseE(1/78)\n",
        "\n",
        "If differs there could be a lot of entropy/disorder present\n"
      ],
      "metadata": {
        "id": "XbzOK2MWu0vu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also we can generate from the model too\n",
        "\n",
        "Note that: self(idx) calls the forward function\n",
        "\n",
        "Update forward function by making target optional target=None to prevent errors in using self(idx) without targets"
      ],
      "metadata": {
        "id": "uqPs4RwpvdbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate using the model above\n",
        "\n",
        "# call the module\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "\n",
        "# pass the inputs\n",
        "logits, loss = m(xb, yb)\n",
        "\n",
        "print(logits.shape)\n",
        "\n",
        "# Now we are able to evaluate the loss\n",
        "print(loss)\n",
        "\n",
        "# indexes\n",
        "# zero is how we will kick off the generation\n",
        "idx = torch.zeros(\n",
        "    (1,1),\n",
        "    dtype=torch.long\n",
        ")\n",
        "# Remember token 0 is the element that represents a new line character\n",
        "# Reasonable thing to feed in as the very first character in a sequence\n",
        "\n",
        "# Ask to generate 100 tokens\n",
        "# some reformatting due to batch dimension required and python list conversion\n",
        "token_list = m.generate(\n",
        "    idx,\n",
        "    max_new_tokens=100\n",
        ")\n",
        "# This is useful to be fed into the decode function\n",
        "# The decode function then converts the integers into text\n",
        "text_stream = decode(token_list[0].tolist())\n",
        "print(text_stream)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Z89A-a3dle5",
        "outputId": "d47b62b3-65c9-4cfb-d2cb-19d020a803e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 78])\n",
            "tensor(5.0773, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "H(:ppt]oqY;ON(Q? !weGCBSaA[i5Q1x5L)FY!ENqBYe*s6bZEg2j\"\n",
            "QN2:px.s[kQMm1\n",
            "uc'(GrDQVBp9kLPyG.XD[Hc29B\n",
            "4tI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Garbage output generation at the moment as the model is just random, no data has been used to update it's weights yet\n"
      ],
      "metadata": {
        "id": "OU-8bkQk8sGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the logits or probabilities can ranked based on smallest probabilities to give a signal for novelty/unusual too which could be useful in unique cases"
      ],
      "metadata": {
        "id": "H37bilJM26rG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Right now to be clear: the model has no understanding of history, it just used the current token to generate the next token"
      ],
      "metadata": {
        "id": "63cdoKS29QLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To at least deal (a bit) with the randomness of the model let's train it"
      ],
      "metadata": {
        "id": "SdkCMuPl9beg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normally learning rate of 1e-4 is recommended but with this small network we can use 1e-3 for now"
      ],
      "metadata": {
        "id": "FptowTAn9_0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a Pytorch optimisation object\n",
        "# Note the simplest possible optimiser is SGD (Stochastic Gradient Descent)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    m.parameters(),\n",
        "    lr=1e-3\n",
        ")"
      ],
      "metadata": {
        "id": "jDQeVQPT1-V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here: the optimiser object, takes the gradients and updates the parameters using gradient descent"
      ],
      "metadata": {
        "id": "Cp9GCzW2-LP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# different batch used here\n",
        "\n",
        "optimisation_batch_size = 32\n",
        "\n",
        "# for some number of steps (ie 100 iterations)\n",
        "for steps in range(100):\n",
        "\n",
        "  # sample a batch of data\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  # evaluate the loss\n",
        "  logits, loss = m(xb, yb)\n",
        "\n",
        "  # zero out all gradients from the previous step\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "  # get the gradients for all the parameters\n",
        "  loss.backward()\n",
        "\n",
        "  # use the gradients to then update the parameters\n",
        "  optimizer.step()\n",
        "\n",
        "  # let's see what kind of losses we get\n",
        "  print(loss.item())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlB3uhji-GAv",
        "outputId": "22553a48-1424-41c5-8723-111fcee37e88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.056305408477783\n",
            "5.082642078399658\n",
            "4.965554714202881\n",
            "5.022189617156982\n",
            "5.155137062072754\n",
            "5.3370561599731445\n",
            "4.950085163116455\n",
            "5.159801959991455\n",
            "5.2557196617126465\n",
            "5.010470867156982\n",
            "5.015458106994629\n",
            "5.010210990905762\n",
            "4.872528553009033\n",
            "5.055800914764404\n",
            "4.686440467834473\n",
            "4.670324802398682\n",
            "5.046802520751953\n",
            "4.776988983154297\n",
            "5.216959476470947\n",
            "5.065333843231201\n",
            "5.00602388381958\n",
            "4.689388275146484\n",
            "4.881626129150391\n",
            "5.342155933380127\n",
            "4.946617126464844\n",
            "4.976146697998047\n",
            "4.8589653968811035\n",
            "4.994383811950684\n",
            "5.263835906982422\n",
            "5.33263635635376\n",
            "4.598924160003662\n",
            "5.157464027404785\n",
            "4.830728054046631\n",
            "4.64491605758667\n",
            "4.7793498039245605\n",
            "4.793103218078613\n",
            "4.792045593261719\n",
            "4.709632396697998\n",
            "4.913875102996826\n",
            "4.958583831787109\n",
            "5.0898518562316895\n",
            "4.6051225662231445\n",
            "4.900825023651123\n",
            "4.715938568115234\n",
            "4.75063943862915\n",
            "4.561678886413574\n",
            "4.674192428588867\n",
            "4.73274564743042\n",
            "4.9364728927612305\n",
            "4.847949981689453\n",
            "5.0041303634643555\n",
            "5.233006954193115\n",
            "4.813653945922852\n",
            "4.669097423553467\n",
            "4.961140155792236\n",
            "4.745000839233398\n",
            "5.113552093505859\n",
            "4.898138046264648\n",
            "4.90980863571167\n",
            "4.673839092254639\n",
            "4.840709686279297\n",
            "4.634303092956543\n",
            "4.78190803527832\n",
            "4.867321968078613\n",
            "4.964488506317139\n",
            "4.526994228363037\n",
            "4.711728572845459\n",
            "4.751303195953369\n",
            "4.949723243713379\n",
            "5.205679416656494\n",
            "4.495652198791504\n",
            "4.72242546081543\n",
            "4.86153507232666\n",
            "4.896749973297119\n",
            "4.9910149574279785\n",
            "4.668844699859619\n",
            "4.585628986358643\n",
            "4.71990442276001\n",
            "4.668457984924316\n",
            "4.903108596801758\n",
            "4.944430828094482\n",
            "4.614198684692383\n",
            "4.618643760681152\n",
            "4.897018909454346\n",
            "4.891502380371094\n",
            "4.875820159912109\n",
            "4.953908443450928\n",
            "5.195980548858643\n",
            "4.694631576538086\n",
            "4.622328281402588\n",
            "4.933272361755371\n",
            "4.755599498748779\n",
            "4.663283824920654\n",
            "5.074558258056641\n",
            "4.99336576461792\n",
            "5.016933917999268\n",
            "4.8415446281433105\n",
            "4.4406819343566895\n",
            "5.133656978607178\n",
            "5.253003120422363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review: started around 5.1\n",
        "ended around 5.3\n",
        "\n",
        "Increase number of iterations and only print at the end"
      ],
      "metadata": {
        "id": "oeB81cX0_hES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# different batch used here\n",
        "\n",
        "optimisation_batch_size = 32\n",
        "\n",
        "# for some number of steps (ie 100 iterations)\n",
        "for steps in range(10000):\n",
        "\n",
        "  # sample a batch of data\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  # evaluate the loss\n",
        "  logits, loss = m(xb, yb)\n",
        "\n",
        "  # zero out all gradients from the previous step\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "  # get the gradients for all the parameters\n",
        "  loss.backward()\n",
        "\n",
        "  # use the gradients to then update the parameters\n",
        "  optimizer.step()\n",
        "\n",
        "  # let's see what kind of losses we get\n",
        "print(loss.item())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edvkgenn_Ula",
        "outputId": "f6bd0461-d3cc-4915-da54-cc7dc19e576e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.551568031311035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Down now to 4.1 (initially - 1000 further iterations) and then with a further 10000+ iterations it get to 2.6"
      ],
      "metadata": {
        "id": "AVuQ1dLkAGK0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if the streamed text is better"
      ],
      "metadata": {
        "id": "CD8mAUjYAnHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros(\n",
        "    (1,1),\n",
        "    dtype=torch.long\n",
        ")\n",
        "# Remember token 0 is the element that represents a new line character\n",
        "# Reasonable thing to feed in as the very first character in a sequence\n",
        "\n",
        "# Ask to generate 100 tokens\n",
        "# some reformatting due to batch dimension required and python list conversion\n",
        "token_list = m.generate(\n",
        "    idx,\n",
        "    max_new_tokens=100\n",
        ")\n",
        "# This is useful to be fed into the decode function\n",
        "# The decode function then converts the integers into text\n",
        "text_stream = decode(token_list[0].tolist())\n",
        "print(text_stream)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4vbRy8qAAWF",
        "outputId": "f7c99fa5-e86b-427e-c001-96761f2b0b63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ". cotot od cout o l ngang henond d --- cal y st s peyoimr 44Le oun ghas arinahurll a ond obemeches p\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's gotten slightly better"
      ],
      "metadata": {
        "id": "290-byBcA6PF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CtpkMQ9xA4OF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}