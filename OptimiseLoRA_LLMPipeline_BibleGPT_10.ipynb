{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshuwaifo/A-Bible-Pre-trained-Transformer-Model/blob/main/OptimiseLoRA_LLMPipeline_BibleGPT_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changing random seed with LoRA doesn't affect performance drastically\n",
        "\n",
        "Changing random seed a good sanity check to ensure approximate reproducibility and not just due to chance"
      ],
      "metadata": {
        "id": "SFNTyx-KrF_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# QLoRA tradeoff\n",
        "\n",
        "# decrease in memory usage: being able to run on a gpu versus not\n",
        "# increase in training time: due to additional quantization and dequantization steps\n",
        "\n",
        "# when compared with LoRA"
      ],
      "metadata": {
        "id": "lD_YtVaGriny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seeing Adam versus SGD\n",
        "\n",
        "Adam adds two additional sets of parameters in comparison\n",
        "\n",
        "So SGD could have been a way to save memory: however the memory saving is minimal with smaller ranks useful for larger ranks ie 256"
      ],
      "metadata": {
        "id": "XwvJsJzwsBxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying LoRA to query and value only\n",
        "# scaling factor alpha = 2*rank\n",
        "# recommending rank of 256"
      ],
      "metadata": {
        "id": "hd4e1S4Zs3b0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full-finetuning not always the best way to go, dataset quality is important"
      ],
      "metadata": {
        "id": "apn8HG14tGni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensemble LoRA weights\n",
        "\n",
        "# LoRA for pretraining: interesting idea"
      ],
      "metadata": {
        "id": "zZQhZLSWtF_6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Developing an LLM (Large Language Model)\n",
        "\n",
        "- Building\n",
        "\n",
        "- Training\n",
        "\n",
        "- Fine-tuning"
      ],
      "metadata": {
        "id": "38Iy24VSRwcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage types (all three have merit and unique value)\n",
        "\n",
        "# - public services ie hugging face\n",
        "# - public proprietary services ie chatgpt\n",
        "\n",
        "# - Running a custom llm locally ie ollama or by using litgpt\n",
        "# Use case: More for me only as a customer, I can run my own LLM and use it\n",
        "\n",
        "# - deploying a custom LLM and using an LLM via a private API ie litgpt-serve\n",
        "# This is deploying on an external server or web service\n",
        "# Use case: Useful and interesting when it comes to developing products\n",
        "# Can also run this locally or on a server (with an API endpoint)\n",
        "# This can be used in an application down the line (ie webapp, phone app, )\n"
      ],
      "metadata": {
        "id": "M0e8vxnSRv0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Open Source\n",
        "- Open Weights"
      ],
      "metadata": {
        "id": "slUbvGnySrLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What does it take to develop an LLM\n",
        "# Pulling the curtain back\n",
        "\n",
        "# Stages\n",
        "\n",
        "\n",
        "# Data preparation\n",
        "# Sampling the dataset\n",
        "# Coding\n",
        "# Attention mechanism (the motor of the LLM)\n",
        "# LLM architecture ()\n",
        "## Building an LLM\n",
        "\n",
        "### Pretraining\n",
        "\n",
        "# Taking a large language model architecture\n",
        "# Training it on a dataset\n",
        "# Training loop\n",
        "# Model evaluation\n",
        "# Mechanism to save and load these pretrained weights\n",
        "# So we can use the Large Language Model later\n",
        "## Forms the Foundation model OR Base model\n",
        "# This isn't normally the end product\n",
        "# It's like an intermediate part of the product but it is super useful too\n",
        "# This model is the data-style blabbing model which can be useful in detecting novelty\n",
        "\n",
        "# This foundation/base model is the base for finetuning\n",
        "\n",
        "### Finetuning\n",
        "\n",
        "# Dataset with class labels --producing--> ie Classifier\n",
        "# More examples: categorising text, identifying spam emails\n",
        "\n",
        "\n",
        "# OR\n",
        "# Instruction dataset --producing--> ie Personal assistant\n",
        "# More example: getting a chatbot"
      ],
      "metadata": {
        "id": "d0gB3cR1UAC9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Book Build a Large Language Model by Sebastian Raschka has the detailed code\n",
        "\n",
        "GitHub repository: https://github.com/rasbt/LLMs-from-scratch"
      ],
      "metadata": {
        "id": "J-xVyaqNWEaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset\n",
        "\n",
        "# Feeding dataset into the Large Language Model\n",
        "\n",
        "# Implementing the LLM architecture"
      ],
      "metadata": {
        "id": "6MIau3vuUW6K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}