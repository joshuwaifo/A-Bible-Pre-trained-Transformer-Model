{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshuwaifo/A-Bible-Pre-trained-Transformer-Model/blob/main/QLoraExample_Walkthrough_WithCode_BibleGPT_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In training: learn how to update the pre-trained weight matrix\n",
        "\n",
        "\n",
        "LoRA: factorise weight update matrix into 2 smaller matrices\n",
        "\n",
        "There will be multiple layers of these in the large language model\n",
        "\n",
        "Example if weight matrix was 10 by 10 = 100 parameters\n",
        "\n",
        "Might be factorised into 10 by 2 and 2 by 10 matrix => 20 + 20 parameters = 40 parameters\n",
        "\n",
        "This approach can help save a lot of the parameters that are needed\n",
        "\n",
        "This also corresponds to the time saved too:\n",
        "\n",
        "Full Finetuning can go from 8 hours plus on multiple GPUs to 1 hour plus on a single GPU with LoRA\n",
        "\n",
        "\n",
        "And memory too:\n",
        "\n",
        "Full finetuning 39+ GB per GPU of memory to 16+ GB on a single GPU with LoRA and almost to 12+ GB with qLoRA\n",
        "\n"
      ],
      "metadata": {
        "id": "zlvaG0LyLi-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CPU offloading transferring parametrs from GPU to CPU"
      ],
      "metadata": {
        "id": "BYhai302M3kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "QLoRA: stands for Quantized LoRA, Quantized Low-Rank Approximation"
      ],
      "metadata": {
        "id": "X5uWuMAPN9jz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To do it in practice here are some ways\n",
        "\n",
        "# Lit-GPT: fully open sourced repository from Lightning AI\n",
        "# Very hackable as code/source code is present"
      ],
      "metadata": {
        "id": "vszhFqtgOEXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup process, probably need to run this elsewhere ie Replit or on my local machine\n"
      ],
      "metadata": {
        "id": "IqwGd_9tOfII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# git clone https://github.com/Lightning-AI/lit-gpt\n",
        "# cd lit-gpt\n",
        "# pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmHf5vocOgmO",
        "outputId": "aa64f46c-6eca-43fe-e018-ee1bb5f08c9b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inside cd lit-gpt contains:\n",
        "\n",
        "- LICENSE\n",
        "- checkpoints\n",
        "- finetune\n",
        "- notebooks\n",
        "- quantize\n",
        "- setup.py\n",
        "- xla\n",
        "- README.md\n",
        "- data\n",
        "- generate\n",
        "- out\n",
        "- requirements.txt\n",
        "- tests\n",
        "- chat\n",
        "- eval\n",
        "- lit_gpt\n",
        "- pretrain\n",
        "- scripts\n",
        "- tutorials"
      ],
      "metadata": {
        "id": "NttxcmIuO6Yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tutorials recommended as a way to go from Sebastian\n",
        "\n",
        "# helps with dealing with out of memory issues etc."
      ],
      "metadata": {
        "id": "7gMwjjJfOlS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After cloning the litGPT repository and installing the requirements:\n",
        "\n",
        "- next: choose a model (ie Falcon, Llama, Mistral, StableLM etc.)\n",
        "\n",
        "- download the model:"
      ],
      "metadata": {
        "id": "ozUFBWUyPgWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of model download (Mistral 7 Billion parameter model)\n",
        "# python scripts/download.py --repo_id mistralai/Mistral-7B-Instruct-v0.1"
      ],
      "metadata": {
        "id": "zM8KI98JQNV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the downloaded weights into an appropriate format"
      ],
      "metadata": {
        "id": "M4OHKDYIQfa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "# python scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/mistralai/Mistral-7B-Instruct-v0.1"
      ],
      "metadata": {
        "id": "3nUKvlcVQmb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare a dataset: this step is necessary with a specific model as each model may have a different tokeniser so make sure things match up\n",
        "\n",
        "The dataset is therefore then prepared using a specific model tokenizer"
      ],
      "metadata": {
        "id": "laQu2xUcQzPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A lot of examples exist on the Lit-GPT pagehere are pretraining and finetuning examples too\n",
        "# And there are tutorials for preparing your own custom dataset\n",
        "\n",
        "# from a CSV file or alpaca type dataset etc.\n",
        "# CSV file: instructions and outputs format\n",
        "\n",
        "# example using the alpaca dataset\n",
        "\n",
        "# python scripts/prepare_alpaca.py --checkpoint_dir checkpoints/mistralai/Mistral-7B-Instruct-v0.1\n"
      ],
      "metadata": {
        "id": "hQmGPY5RQ0Ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finetuning step\n",
        "\n",
        "Example below is by using\n",
        "\n",
        "- LoRA\n",
        "- 16 bit brain float precision (this helps a lot with memory constraints and computation time in comparison to using 32 bits without sacrificing too much in performance)\n",
        "- Quantization also used here using normal float 4 (to save memory on the order of gigabytes, but sometimes takes a few percentages more computation time)\n",
        "\n",
        "This is effectively QLoRA\n",
        "\n",
        "Then depending on GPU the model would train for a while, seeing things like milliseconds per iteration\n",
        "\n",
        "Total time could be on the order of an hour to a few hours\n",
        "\n",
        "Which isn't bad for doing 50000 iterations"
      ],
      "metadata": {
        "id": "Qdndo5weS1Wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example for doing QLoRA finetuning\n",
        "# python finetune/lora.py --checkpoint_dir checkpoints/mistralai/Mistral-7B-Instruct-v0.1 --precision bf16-true --quantize \"bnb.nf4\""
      ],
      "metadata": {
        "id": "_Fx1V6ISVdnM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more efficient techniques of finetuning ie\n",
        "\n",
        "- finding what a good dataset to train on is\n",
        "\n",
        "- useful tweaks to LoRA\n",
        "\n",
        "Check out: NeurIPS Large Language Model Efficiency Challenge:\n",
        "\n",
        "- 1 LLM + 1 GPU + 1 Day"
      ],
      "metadata": {
        "id": "40lId2ZwVwnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lit-GPT is the official starter kit to emphasise validity\n",
        "\n",
        "# https://llm-efficiency-challenge.github.io/\n",
        "\n",
        "# https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/neurips_challenge_quickstart.md\n",
        "\n"
      ],
      "metadata": {
        "id": "6YXuY1qMWIse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning more about the instructor that motivated this:\n",
        "\n",
        "- @rasbt\n",
        "- sebastian@lightning.ai\n",
        "- https://sebastianraschka.com\n",
        "- https://lightning.ai"
      ],
      "metadata": {
        "id": "NuwKREuOWhnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lightning AI: Creators of PyTorch Lightning"
      ],
      "metadata": {
        "id": "vOPoxWCZVuLD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretraining and Finetuning pipeline\n",
        "\n",
        "Large unlabelled general text corpus\n",
        "\n",
        "Pretrained LLM also known as Foundation model (this is normally as practitioners downloaded from a model hub or something similar)\n",
        "\n",
        "If we have a specific problem or domain ie Medical or Finance\n",
        "\n",
        "- we might train the LLM further on a domain specific data\n",
        "\n"
      ],
      "metadata": {
        "id": "L8Ks5fpgXNbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# There are many ways of instruction finetuning\n",
        "\n",
        "# For example:\n",
        "# supervised instruction finetuning\n",
        "# Reinforcement Learning with Human Feedback\n",
        "# Direct Preference Optimisation"
      ],
      "metadata": {
        "id": "Wzg2WufbXM6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In supervised instruction finetuning\n",
        "\n",
        "we have a dataset consisting of instruction - input (optional) - output\n",
        "\n",
        "To make this more efficient:\n",
        "\n",
        "Use Paraemter Efficient Finetuning techniques like (Q)LoRA\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DxBMZ2-sYdo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imagine the regular finetuning setup where\n",
        "\n",
        "# You want to update the pretrained weights\n",
        "\n",
        "# Imaging weight update was saved separately\n",
        "\n",
        "# Then you can think of LoRA as a Low-rank approximation or decomposition of that separately saved weight matrix\n",
        "\n",
        "# So instead of learning a full weight update matrix\n",
        "\n",
        "# LoRA is learning the decomposed matrices involved in the replacement process\n",
        "\n",
        "# The trick being we have this rank and we can choose a very small rank for these matrices\n",
        "\n",
        "# Which helps one to save a lot of parameters\n",
        "\n",
        "# Making the training more efficient\n",
        "\n"
      ],
      "metadata": {
        "id": "rCOCc76-Y8dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding: with low rank decomposition we can approximate the results of the full weight update"
      ],
      "metadata": {
        "id": "7AVN9HGBY8E-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In practice there are a lot of settings/tweaks/hyperparameters we can change\n",
        "\n",
        "# For example:\n",
        "\n",
        "# what layers you want to apply LoRA\n",
        "# scaling factor alpha\n",
        "\n",
        "\n",
        "lora_r = 8\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.05\n",
        "\n",
        "# query, key and value of an attention head\n",
        "lora_query = True\n",
        "lora_key = False\n",
        "lora_value = True\n",
        "lora_projection = False\n",
        "lora_mlp = False\n",
        "lora_head = False\n",
        "\n",
        "# Lessons from running 100s if not 1000s of experiments"
      ],
      "metadata": {
        "id": "klVrK62taWaE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}