{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshuwaifo/A-Bible-Pre-trained-Transformer-Model/blob/main/Align_LLMEvaluation_BibleGPT_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data\n",
        "\n",
        "LLM Receiveing data"
      ],
      "metadata": {
        "id": "SaoQ4XsAzJOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (LLM) Architectures\n",
        "\n",
        "# Original GPT model\n",
        "\n",
        "# GELU activation\n",
        "# Feedforward (2 Linear with a non linearity)\n",
        "# Transformer block repeated multiple times (12 to 32/64 times)\n",
        "# Masked multi-head attention\n",
        "# RMS or Layer Norm\n",
        "# Positional embedding layer\n",
        "# Token embedding layer\n"
      ],
      "metadata": {
        "id": "oOYQQIkBzM53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Total number of parameters\n",
        "\n",
        "124M -> 1558M\n",
        "\n",
        "Transformer block repetitions\n",
        "\n",
        "12 -> 48\n",
        "\n",
        "Embedding dimensions:\n",
        "\n",
        "768 -> 1600 -> 8000 (apparently)\n",
        "\n",
        "Number of heads in multi-head attention mechanism (equivalent to stacked channels in convolution layer):\n",
        "\n",
        "12 -> 25"
      ],
      "metadata": {
        "id": "oa75cJIWzt4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT-2 large\n",
        "\n",
        "# Llama 2 7B:\n",
        "\n",
        "# RMS norm: Root Mean Square norm\n",
        "# Works better for multi-gpu training\n",
        "# batch size independent\n",
        "\n",
        "# SILU vs GELU activation\n",
        "\n",
        "# RoPE (rotational - relative) positional embedding instead of Sinusoidal/Absolute positional embedding\n"
      ],
      "metadata": {
        "id": "j7X3V3Piz92k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most LLMs are still similar\n",
        "\n",
        "Pretraining\n",
        "\n",
        "- Creates the foundation model\n",
        "\n",
        "Pretty standard deep learning training loop (similar to watch is used in MLP and CNNs)\n",
        "\n",
        "- Adam optimiser, learning rate scheduler: 1 cycle cosine schedule, cross entropy loss\n",
        "\n",
        "- Main difference is larger scale (ie multiple GPUs)\n",
        "\n"
      ],
      "metadata": {
        "id": "-lZh9tpc0_bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Labels/targets are the inputs shifted by +1\n",
        "# Training for ~1-2 epochs\n",
        "# Epoch is one pass over the training set\n",
        "# As training set is typically large\n",
        "# and distributed over many machines\n",
        "# It's not super feasible to do classic epochs\n",
        "# It's more like drawing random data from the dataset\n",
        "# which may lead to collisions, repeated batches, batches not seen etc"
      ],
      "metadata": {
        "id": "Yk2z1iS11n6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pythia from Eleuther AI\n",
        "\n",
        "After 2 epochs typically see overfitting\n",
        "\n",
        "Large gap between validation and training loss\n",
        "\n",
        "Memorisation issues\n"
      ],
      "metadata": {
        "id": "eeJ68mEZ2CUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# When implementing generation we have settings like\n",
        "\n",
        "# top-k\n",
        "# top-p\n",
        "# temperature scaling (controlling amount of randomness)"
      ],
      "metadata": {
        "id": "KS5uEikS2k0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Masking certain things that are likely to be memorised\n",
        "\n",
        "We want it to reason though\n"
      ],
      "metadata": {
        "id": "1yLTG4622xRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretrain LLM - TinyLlama 1.1B on 1 billion tokens - 4 weeks\n",
        "\n",
        "# If interested in pretraining, not necessary in most cases\n",
        "# check out:\n",
        "# https://lightning.ai/lightning-ai/studios/pretrain-llms-tinyllama-1-1b"
      ],
      "metadata": {
        "id": "PIp17NKc3wVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the time when we work with LLMs, we work with pre-trained weights (that someone has kindly enough shared openly)"
      ],
      "metadata": {
        "id": "vzTgn3J34AMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading pretrained weights (more than 20 LLMs):\n",
        "\n",
        "# Llama 3 - Meta AI\n",
        "# Llama 2 - Meta AI\n",
        "# Code Llama - Meta AI\n",
        "# Mixtral MOE - Mistral AI\n",
        "# Mistral - Mistral AI\n",
        "# CodeGemma - Google\n",
        "\n",
        "# https://github.com/Lightning-AI/litgpt"
      ],
      "metadata": {
        "id": "IO26fec84O17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code base is left readable\n",
        "\n",
        "litgpt [action] [model]\n",
        "[action]:\n",
        "- download\n",
        "- chat\n",
        "- finetune\n",
        "- pretrain\n",
        "- serve (deploy it)\n",
        "\n",
        "[model]:\n",
        "- meta-llama/Meta-Llama-3-8B-Instruct\n"
      ],
      "metadata": {
        "id": "DGFeX-Te4j6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finetuning\n",
        "\n",
        "# Adapting foundation model for downstream tasks\n",
        "\n",
        "# Classifier\n",
        "# Label, Text\n",
        "# Replace output layer\n",
        "# Original GPT output layer had 50,257 units\n",
        "# This is also the number of tokens in the vocabulary\n",
        "# input nodes -> output nodes\n",
        "\n",
        "\n",
        "# Personal assistant"
      ],
      "metadata": {
        "id": "sZTTsbTf45W4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace number of output layers\n",
        "\n",
        "Look at task specific performance\n",
        "\n",
        "For example\n",
        "- classification accuracy\n",
        "\n",
        "Accuracy is not differentiable\n",
        "Hence why we use loss"
      ],
      "metadata": {
        "id": "-VFBVHYb-1s7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We don't need to fine tune all the layers\n",
        "\n",
        "# Updating last 2 layers and last 2 transformer blocks\n",
        "\n",
        "# Faster too\n",
        "\n",
        "# Most business tasks are classification tasks\n",
        "# Classification finetuning"
      ],
      "metadata": {
        "id": "Qi_hKN91-zfs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instruction finetuning\n",
        "\n",
        "Instruction dataset\n",
        "\n",
        "- Instruction (could append input here too)\n",
        "- Input (optional)\n",
        "- Output"
      ],
      "metadata": {
        "id": "ybJcoEXu_eBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply a prompt style template\n",
        "# For example, Alpaca-style prompt template\n",
        "\n",
        "# Feed template to LLM"
      ],
      "metadata": {
        "id": "fTK6Ejdu_dRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset size for instruction dataset\n",
        "\n",
        "Thousands of examples (1k -> 50k -> 100k)\n",
        "\n",
        "Quality is important"
      ],
      "metadata": {
        "id": "jB6XVpz-AGUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preference tuning\n",
        "\n",
        "# Refine LLM responses\n",
        "# Alignment / steering ie safety, helpfulness\n",
        "\n",
        "\n",
        "# Prompt dataset\n",
        "# Preference dataset\n",
        "\n",
        "# Techniques"
      ],
      "metadata": {
        "id": "KFAcdHvHAP8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating LLMs\n",
        "\n",
        "Instruction fine tuned models\n",
        "\n",
        "Preference fine tuned models\n",
        "\n",
        "- MMLU: 0 - 100 (most popular)\n",
        "Measuring Massive Multitask Language Understanding - 2020 paper\n",
        "\n",
        "Multiple choice questions from diverse subjects (Multople choice questions) - really just memorisation\n",
        "\n",
        "- LM Evaluation Harness\n",
        "EleutherAI\n",
        "\n",
        "\n",
        "- AlpacaEval\n",
        "Measure conversational performance of an LLM\n",
        "\n",
        "Compare given LLM to performance of GPT-4 preview and uses GPT-4 based to ask which is better"
      ],
      "metadata": {
        "id": "ZYV9RvVAFQ3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate MMLU and other benchmarks using LitGPT\n",
        "\n",
        "litgpt evaluate checkpoints/microsoft/phi-2/ --batch_size 4 --tasks \"hellaswag,truthfulqa,_mc2,mmlu\" --out_dir evaluate_model/"
      ],
      "metadata": {
        "id": "8LdlaX_uGE9j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}