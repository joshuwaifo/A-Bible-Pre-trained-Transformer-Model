{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshuwaifo/A-Bible-Pre-trained-Transformer-Model/blob/main/IPAIDotMe_BusinessPlan__BibleGPT_14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Labels required and gathered by expert or users"
      ],
      "metadata": {
        "id": "flKtZJd4_eMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applications of LLMs\n",
        "\n",
        "# Novelty generation\n",
        "# Analysis"
      ],
      "metadata": {
        "id": "X2kz3eyvCW0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "User and AI system interaction\n",
        "\n",
        "- Knowledge retrieval\n",
        "\n",
        "Medicine\n",
        "Law\n",
        "\n",
        "- Sifting through documents\n",
        "- Answering technical questions\n",
        "- Summarising lengthy passages"
      ],
      "metadata": {
        "id": "2vGSIyxtCv-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stages of building and using LLMs\n",
        "\n",
        "# Understand the mechanics and limitations\n",
        "\n",
        "# Pretraining\n",
        "# Finetuning\n",
        "\n",
        "# Own domain-specific datasets or tasks\n",
        "\n",
        "# Going from general purpose to domain specific\n",
        "# Custom built LLMs\n",
        "# Tailored for specific tasks or domains"
      ],
      "metadata": {
        "id": "g42PUOUODIEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BloombergGPT: finance\n",
        "\n",
        "medicine"
      ],
      "metadata": {
        "id": "8_EW1pvcDkQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom built LLMs\n",
        "\n",
        "# to handle data privacy\n",
        "# deploying directly on customer devices like laptops and smartphones ie Apple\n",
        "\n",
        "# local implementation\n",
        "# decrease latency\n",
        "# reduce server-related costs\n",
        "\n",
        "# complete autonomy\n",
        "# control updates and modifications to the model as and when needed"
      ],
      "metadata": {
        "id": "dDEOikbsDjw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM\n",
        "\n",
        "- pretraining (and continued)\n",
        "- finetuning (instruction, preference)\n",
        "\n",
        "Pretraining: develop a broad understanding of X: ie language, vision etc\n",
        "- next word prediction on large text datasets (ie trillions of words in total)\n",
        "- text completion\n",
        "- novelty analysis\n",
        "- few shot capabilities\n",
        "\n",
        "Finetuning: refinement to more specific tasks or domains\n",
        "- classification\n",
        "- summarisation\n",
        "- translation\n",
        "- personal assistant\n",
        "\n",
        "See figure 1.3 in Build a Large Language Model (From Scratch) book by Sebastian R."
      ],
      "metadata": {
        "id": "NJ4TpGM0EDBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PRETRAINING\n",
        "# Train on raw text (large corpus of text data)\n",
        "# Raw = regular (filtered) text no labelling information\n",
        "# filtering techniques include: removing formatting characters or documents in unknown languages\n",
        "# creates a BASE or FOUNDATION model ie GPT-3\n",
        "\n",
        "# Typically has limitations\n",
        "# Good at few-shot examples\n",
        "\n",
        "# Results in a pretrained BASE or Foundation LLM"
      ],
      "metadata": {
        "id": "DjikTvyhFLog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finetuning\n",
        "\n",
        "Instruction finetuning\n",
        "- instruction and answer pairs ie query to translate a text, accompanied with correctly translated text\n",
        "\n",
        "Classification task fine tuning\n",
        "- text and associated class labels"
      ],
      "metadata": {
        "id": "pXKXawizF2vC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer architecture\n",
        "\n",
        "# 2017 paper: Attention Is All You Need\n",
        "# Originally developed for machine translation English to German and English to French\n",
        "\n",
        "# Encoder: processes input text and produces embedding representation\n",
        "# Numerical representation that captures many different factors in different dimensions\n",
        "\n",
        "# Decoder: generates translated text one word at a time\n",
        "\n",
        "# Self, Causal, Masked-Causal, Multi-head attention\n",
        "\n",
        "# Input text to be translated ie \"This is an example\"\n",
        "# Input text prepared for the encodeer: preprocessing steps\n",
        "# Encoder has access to the complete input text\n",
        "# Can therefore produce text encoding that can be used by the decoder\n",
        "# Encoder returns embedding vectors as input to the decoder\n",
        "\n",
        "# A partial output text (the model completes the translation one word at a time) ie Das ist ein\n",
        "# This serves as input text to the decoder stage\n",
        "# This input text is prepared for the decoder: Preprocessing steps\n",
        "# Decoder generates the translated text one word at a time\n",
        "# Output layers\n",
        "# Complete output (translation) ie Das ist ein Beispiel\n",
        "\n",
        "# Summary:\n",
        "# 2 submodules\n",
        "# encoder: produce vectors captures contextual information of the input\n",
        "# decoder: generate output text by also taking into account the encoded vectors\n",
        "\n",
        "# Encoder and decoder consist of many layers connected by a self-attention mechanism"
      ],
      "metadata": {
        "id": "ZX69UDdVF1Bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self-attention mechanism\n",
        "\n",
        "- Key component of transformers and some LLMs\n",
        "- allows model to weigh the importance of different tokens in a sequence relative to each other ie sub-words relative to other sub-words, image-patches relative to other image-patches\n",
        "\n",
        "This enables the model to capture long-range dependencies and contextual relationships within the input data/datum\n",
        "- enabling ability to generate coherent and contextually relevant output data/datum\n"
      ],
      "metadata": {
        "id": "cUJX4kXCIpwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variants of the Transformer Architecture\n",
        "\n",
        "# BERT: Bidirectional Encoder Representations from Transformers\n",
        "# Built upon original transformer's encoder submodule\n",
        "# Different training approach to GPT\n",
        "# Specialise in masked word prediction\n",
        "# This is where the model predicts hidden tokens in a sequence\n",
        "# For example, hidden words in a given sentence\n",
        "# A strength for tasks like text classification, document categorisation, sentiment prediction, detecting toxic content\n",
        "\n",
        "# Fill in the missing words to generate the original sentence\n",
        "# By receiving inputs where words are randomly masked during training\n",
        "\n",
        "\n",
        "# GPT: Generative Pretrained Transformers\n",
        "# designed for Generative tasks and producing coherent text sequences\n",
        "# Built upon original transformer's decoder submodule\n",
        "\n",
        "# Learn to generate one word at a time\n",
        "# Receives incomplete texts\n",
        "\n",
        "# Great at Zero-shot and few-shot learning tasks"
      ],
      "metadata": {
        "id": "bDgqdG7DIpSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zero-shot learning:\n",
        "- Generalise to completely unseen tasks\n",
        "- No prior specific examples\n",
        "- No example of target provided within the (input) prompt\n",
        "\n",
        "Few-shot learning:\n",
        "- Learning from a minimal number of examples that the user provides as input\n",
        "- Provide examples of the target within the (input) prompt"
      ],
      "metadata": {
        "id": "O9RdbOYSK969"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create plausible text (completion) given a partial input text (prompt)\n",
        "# Zero-shot: Complete a task within an explicit target example (this differs from providing the instruction)\n",
        "# Few-shot: Complete a task given a few target examples of the task\n"
      ],
      "metadata": {
        "id": "bLpr2eaxK9fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers versus LLMs\n",
        "\n",
        "- Transformers used in Computer Vision as well as Natural Language Processing\n",
        "\n",
        "- Other types of LLMs outside of transformers ie Convolutional, Recurrent, State Sequence etc.\n",
        "\n",
        "Motivation: improve computational efficiency of LLMs"
      ],
      "metadata": {
        "id": "ueaFpXWDL8mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM here: Transformer based LLMs similar to GPT"
      ],
      "metadata": {
        "id": "JQzV1MTXL8OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilising large datasets\n",
        "\n",
        "For GPT and BERT like models\n",
        "- Used diverse and comprehensive text corpora containing billions of words\n",
        "- Included natural and computer languages"
      ],
      "metadata": {
        "id": "sGe8rMnGMsZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Self-supervised learning: Pretraining LLMs\n",
        "# Supervised learning\n",
        "# Unsupervised learning"
      ],
      "metadata": {
        "id": "tF_yvEzSNB4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset name\n",
        "\n",
        "Dataset description\n",
        "\n",
        "Number of tokens\n",
        "\n",
        "Proportion in training data"
      ],
      "metadata": {
        "id": "VR-ChgIKfMUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Token: unit of text/visual that a model reads/sees\n",
        "# Number of tokens roughly equal to number of words and punctuation characters in the text\n",
        "# TOKENIZATION: process of convert text/visual into tokens\n"
      ],
      "metadata": {
        "id": "fDpOtDWwfRp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scale and diversity of dataset\n",
        "\n",
        "- allows for good performance on diverse tasks\n",
        "\n",
        "For example:\n",
        "- language syntax\n",
        "- language semantics\n",
        "- language context\n",
        "- general knowledge"
      ],
      "metadata": {
        "id": "q-xmMlqAfzum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT-3 dataset details\n",
        "\n",
        "# 60:40% split\n",
        "\n",
        "# 410 billion tokens equivalent to 570GB of storage\n",
        "# 1 billion tokens equivalent to 1.5GB of storage (ratio)\n",
        "\n",
        "# Arxiv research papers\n",
        "# Stack exchange code related Q&As\n",
        "\n",
        "# Taglines for Businesss\n",
        "# Simulate the dream, extract the IP\n",
        "# Your internal question and answers and expertise\n",
        "# Powered and served to anyone around the world using research-backed AI"
      ],
      "metadata": {
        "id": "E8AZZFpWgCdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Publically available open dataset of 3 trillion tokens for LLM Pretraining:\n",
        "\n",
        "- Dolma: an Open Corpus of Three Trillion Tokens for LLM Pretraining Research by Soldaini et al. 2024\n",
        "\n",
        "https://arxiv.org/abs/2402.00159\n",
        "\n",
        "This collection may contain copyrighted words and the exact usage terms depends on the country and use case"
      ],
      "metadata": {
        "id": "JK-nKCK7hLlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretrained nature of these models\n",
        "# encourages versatility for further\n",
        "# finetuning on downstream tasks\n",
        "# hence why they are known as base or foundation models\n",
        "\n",
        "# Also pretraining LLMs require access to\n",
        "# significant resources\n",
        "# very expensive\n",
        "# GPT-3 pretraining cost an estimate $4.6 million of cloud computing credits"
      ],
      "metadata": {
        "id": "okngm6f2hvIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are open-source pretrained LLMs available\n",
        "- general purpose tools\n",
        "\n",
        "\n",
        "Pretrain an LLM for educational purposes\n",
        "\n",
        "Computations executable on consumer hardware\n",
        "\n",
        "After implementing\n",
        "- we will also reuse openly available model weights\n",
        "- load them into an architecture of our choice\n",
        "- thus bypassing the need for the expensive pretraining stage\n",
        "- and being helpful for the finetuning processes done later on"
      ],
      "metadata": {
        "id": "zwF6a-rbiMiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT architecture\n",
        "\n",
        "# GPT: Generative Pretrained Transformer\n",
        "# Introduced in 2018 paper by Radford et al at OpenAI\n",
        "# Improving Language Understanding by Generative Pre-Training\n",
        "\n",
        "# GPT-3 is a scaled up version of this model from the paper\n",
        "# More parameters\n",
        "# Larger dataset\n",
        "\n",
        "# ChatGPT original model\n",
        "# Finetuned GPT-3 on a large instruction dataset\n",
        "# Using a method from OpenAI's Instruct GPT paper\n",
        "# Finetuning with Human Feedback to follow instructions\n",
        "\n",
        "# Competent text completion models\n",
        "# Carry out tasks like\n",
        "# Spelling correction\n",
        "# Classification\n",
        "# Language translation\n",
        "\n",
        "# Remarkable given GPT models are\n",
        "# pretrained on a relatively simple task:\n",
        "# next-word prediction task"
      ],
      "metadata": {
        "id": "gWYQAw-Gi4l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next-word pretraining task\n",
        "- GPT models\n",
        "- system learns to predict upcoming word in a sentence\n",
        "- by looking at words that have come before it\n",
        "\n",
        "Helps model understand how words and phrases typically fit together in language\n",
        "\n",
        "Forming a foundation that can be applied to various other tasks\n",
        "\n",
        "The model is simply trained to predict the next\n",
        "-> word"
      ],
      "metadata": {
        "id": "Xqk70Wz-j5u6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Next word prediction task\n",
        "# Form of self-supervised learning\n",
        "# Form of self-labelling\n",
        "\n",
        "# No need to collect labels for training data explicitly\n",
        "# Can leverage structure of the data itself\n",
        "\n",
        "# For example:\n",
        "# can use the next word in a sentence or document\n",
        "# as the label that the model is supposed to predict\n",
        "\n",
        "# Allows us to create labels on the fly\n",
        "# by leveraging massive unlabeled text datasets to train LLMs"
      ],
      "metadata": {
        "id": "tmtY7e67kkUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT architecture relatively simpler to original transformer architecture\n",
        "\n",
        "Decoder part without the encoder"
      ],
      "metadata": {
        "id": "mIxenk2YldYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder style models ie GPT\n",
        "# generate text\n",
        "# Predicting text one word at a time\n",
        "# Considered a type of autoregressive model"
      ],
      "metadata": {
        "id": "9b2RL0DClq31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autoregressive models incoporate their previous outputs as inputs for future predictions\n",
        "\n",
        "In GPT\n",
        "- each new word is chosen based on the sequence that precedes it\n",
        "- improving coherence of the resulting text"
      ],
      "metadata": {
        "id": "rnjWa0s5l4FG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# However, GPT-3 architecture significantly larger\n",
        "# than original transformer model\n",
        "\n",
        "# Original transformer repeated the encoder and decoder blocks 6 times\n",
        "# GPT-3 has 96 transformer layers and 176 billion parameters in total\n",
        "\n"
      ],
      "metadata": {
        "id": "Ab7ZOZaZmI1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT architecture\n",
        "\n",
        "- designed for unidirectional (ie left-to-right) processing\n",
        "- well suited for text generation and next word prediction tasks\n",
        "- generative text in iterative fashion (one word at a time)"
      ],
      "metadata": {
        "id": "c3RAtR7CmeAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT-3 introduced in 2020\n",
        "# By standards of deep learning and large language model development\n",
        "# considered a long time ago\n",
        "\n",
        "# Try to integrate Meta's Llama model (more recent architecture)\n",
        "# Same concept minor modifications"
      ],
      "metadata": {
        "id": "qO3rDScnm-wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT models can also do translation tasks despite decoder only architecture\n",
        "\n",
        "This was a surprise to researchers\n",
        "\n",
        "Emergent behaviour: ability to perform tasks a model wasn't explicitly trained to perform\n",
        "\n",
        "Emerges as a natural consequence of the model's exposure to vast quantities of multilingual data in diverse contexts\n",
        "\n",
        "GPT models learnt the translation patterns betweeen languages\n",
        "\n",
        "Showcasing benefits of large-scale generative language models"
      ],
      "metadata": {
        "id": "-HbZPbWnndfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a large language model\n",
        "\n",
        "# Coding one from scratch\n",
        "# Using GPT as the blueprint\n",
        "\n",
        "# Data preparation process\n",
        "# Implementing the LLM archtecture\n",
        "# Pretraining an LLM\n",
        "# Having a foundation model\n",
        "# Finetuning a foundation model\n",
        "# Either as a personal assistant\n",
        "# Or as a text classifier\n",
        "\n"
      ],
      "metadata": {
        "id": "Ie7B6gMboI8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stage 1\n",
        "\n",
        "1. Data preparation\n",
        "2. Data sampling\n",
        "3. Attention mechanism\n",
        "4. LLM architecture\n",
        "\n",
        "Building an LLM\n",
        "\n",
        "Implement the data sampling and understand the basic mechanism\n",
        "\n",
        "5. Pretraining\n",
        "6. Training loop\n",
        "7. Model evaluation (of LLMs, essential for developing capable NLP systems)\n",
        "8. Load pretrained weights\n",
        "\n",
        "Foundation model\n",
        "\n",
        "Pretrain the LLM on unlabeled data to obtain a foundation model for further finetuning\n",
        "\n",
        "Stage 3 A\n",
        "\n",
        "9. Finetuning\n",
        "\n",
        "Finetune the pretrained LLM to create a classification model\n",
        "\n",
        "\n",
        "\n",
        "Dataset with class labels\n",
        "\n",
        "Producing a classifier\n",
        "\n",
        "Stage 3 B\n",
        "\n",
        "\n",
        "10. Finetuning\n",
        "\n",
        "Finetune the pretrained LLM to create a personal assistant or chat model\n",
        "\n",
        "\n",
        "- answering queries\n"
      ],
      "metadata": {
        "id": "GMMvzzJ7oj9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretraining an LLM from scratch\n",
        "# For GPT-like models cost thousands to millions of dollars in computing costs"
      ],
      "metadata": {
        "id": "oiQQfmTYsiLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training for education purposes on a small dataset\n",
        "\n",
        "- As well as loading openly available model weights"
      ],
      "metadata": {
        "id": "ZiuMTR4Zsh7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary:\n",
        "\n",
        "# LLMs transformed the field of NLP (Natural Language Processing)\n",
        "# previously field relied on:\n",
        "# explicit rule-based systems\n",
        "# simpler statistical methods\n",
        "\n",
        "# LLMs\n",
        "# Deep learning driven approach\n",
        "# Advancements in:\n",
        "# Understanding\n",
        "# Generating\n",
        "# Translating human language"
      ],
      "metadata": {
        "id": "2gStHXLbshpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer architecture\n",
        "\n",
        "- attention mechanism\n",
        "- gives LLM selective access to the whole input sequence\n",
        "- when generating the output one word at a time\n",
        "\n",
        "\n",
        "- encoder: parsing text\n",
        "- decoder: generating text"
      ],
      "metadata": {
        "id": "scx9B6Vhv5hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Working with Text Data\n",
        "# Preparing text for LLM pretraining\n",
        "# Splitting text into word and subword tokens\n",
        "# Byte pair encoding\n",
        "# Sampling training examples with a sliding window approach\n",
        "# Converting tokens into vectors that feed into an LLM\n",
        "\n",
        "# text -> word/subword tokens -> vectors -> LLM\n"
      ],
      "metadata": {
        "id": "FLQAUcthv5x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the training dataset\n",
        "\n",
        "- data preparation pipeline\n",
        "- sampling pipeline\n",
        "\n",
        "Provides the LLM with the text data for pretraining"
      ],
      "metadata": {
        "id": "1p0gJXebyPOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare input text for training LLMs\n",
        "# Splitting text into individual word and subword tokens\n",
        "# These are then encoded into vector representations for the LLM\n",
        "\n",
        "# Advanced tokenisation schemes\n",
        "# Byte pair encoding utilised in popular LLMs like GPT and Llama3.1\n",
        "\n",
        "# Sampling and data loading strategy\n",
        "# Produce input-output pairs necessary\n",
        "# for training LLMs"
      ],
      "metadata": {
        "id": "y5VkfbFVyP9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding Word Embeddings\n",
        "- Deep Neural Network models including LLMs cannot process raw text directly\n",
        "\n",
        "As text is categorical\n",
        "- this makes it incompatible with mathematical operations used to implement and train neural networks"
      ],
      "metadata": {
        "id": "kf_a8vTdzWRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Represent words\n",
        "# Continue from 2.29 Chapter 2"
      ],
      "metadata": {
        "id": "SB8qKxOczWiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Website creator (ipai.me)\n",
        "\n",
        "(/)\n",
        "\n",
        "\"Simulate the dream, extract the IP\"\n",
        "\n",
        "Business service offerings:\n",
        "\n",
        "- Creating your own personal assistant\n",
        "\n",
        "- Personal AI coach for employee training/development, competitive gaming etc.\n",
        "\n",
        "\n",
        "- Novel idea evaluation within specific domains ie IP, Patents, Research papers, Grants, Bids (/product)\n",
        "\n",
        "- Extracting valuable information via data (images, videos, audio, text files etc.). For example, identifying ultra high net worth individuals and investment patterns using public and private information\n",
        "\n",
        "- AI audit of existing business for sole-traders\n",
        "\n",
        "Research backed and money-back guarantee\n",
        "\n",
        "Example: Loom video\n",
        "\n",
        "Testimonials (/testimonials):\n",
        "\n",
        " \"Xcharta is a data extraction start-up leveraging cutting-edge deep learning and computer vision techniques to turn visually represented data in the form of charts into liquid, useable numbers. Joshua, CTO of 88 Labs, worked with Xcharta over the course of 4 months. Primarily, he developed an object detection deep learning model to identify and classify charts in documents. He also collaborated with external third parties to operationalize the model into our Vertex AI endpoint, enhancing communication and value.\" - CEO of Xcharta.\n",
        "\n",
        " \"I got chills - this is so magnificent! Just finished the demo… This system stands out as a unique solution tailored to our industry. Its implementation will not only save time but also serve as an essential element, enabling us to support more clients and their missions efficiently.\" – Principal of Unboxed Philanthropy Advisors.\n",
        "\n",
        "\n",
        "\"..at a high level, this looks like serious work...\" - Technical Advisor, Schmidt Futures\n",
        "\n",
        "Loom video 1\n",
        "Loom video 2\n",
        "Unboxed philanthropy testimonial\n",
        "Schmidt Futures testimonial\n",
        "5th in the World\n",
        "Osas, deep.meta / Zidan /\n",
        "\n",
        "About (/uwaifo):\n",
        "\n",
        "I recently finished building an AI startup, which reached product-market fit, though margins were tight. I handed it over to my co-founder and am now back in the game, looking for the right opportunity. I also continue to mentor founders and decision-makers, offering free guidance on leveraging research-backed AI products and solutions.\n",
        "\n",
        "Here's a brief intro:\n",
        "I'm a former professional FIFA player who developed an AI coaching assistant that helped me rank 5th in the world. Over the past 10+ years, I've built several AI products and assets. I also enjoy researching and building Large Multimodal Agentic Systems from scratch, as well as leveraging robust, well-tested tools.\n",
        "\n",
        "For more details, my LinkedIn profile includes a detailed bio and testimonials. I’m happy to share my CV as well, and here are 2 quick videos that might help provide more context:\n",
        "\n",
        "\n",
        "https://www.loom.com/share/4b7aead31ffe47acae5adca21d9f4a38?t=5\n",
        "https://www.loom.com/share/dd0e9e682d444e0db50927b024b53c8c?sid=f8e5f368-4720-4ca3-89d9-e6b23c1c86f4\n",
        "\n",
        "\n",
        "CV (/ojresume):\n",
        "\n",
        "I offer decision-makers free AI audits and develop solutions that save time or create assets within 100 days, backed by a money-back guarantee.\n",
        "\n",
        "My clients, spanning the UK, UAE, and the US, include organisations such as Eric Schmidt's Moonshot Fund, Schmidt Futures and Cleveland Clinic Abu Dhabi.\n",
        "\n",
        "Personal Highlights:\n",
        "\n",
        " - Developed an AI system for recommending Premier League transfer decisions, resulting in a publication at the ICUR 2017 conference.\n",
        "\n",
        " - Built an AI coaching system for playing FIFA, achieving a Top 5 ranking in the world in FIFA 21 PS4 Seasons out of 3 million active players, and won a cash prize in the monthly (ESL Play) Final.\n",
        "\n",
        " - Created successful demos of an Augmented Reality Single-Camera Sports Broadcast solution for various Move.ai clients, including:\n",
        " -- Football: Premier League (IMG)\n",
        " -- Formula E\n",
        " -- Cricket: ECB\n",
        " -- Martial Arts: One Championship\n",
        "\n",
        " - Contributed to Move.ai's AI system for recreating human motion, utilized in video games by EA Sports and the FIFA Club World Cup.\n",
        "\n",
        " - Invited panel speaker to the Her Tech Talent in AI Network event by PwC UK.\n"
      ],
      "metadata": {
        "id": "DjgS953xpUnt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E98P_GJgrVAD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}