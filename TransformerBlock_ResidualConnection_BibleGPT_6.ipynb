{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshuwaifo/A-Bible-Pre-trained-Transformer-Model/blob/main/TransformerBlock_ResidualConnection_BibleGPT_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previously on BibleGPT 1-5\n",
        "\n",
        "1 minute 2 seconds run time CPU Google Colab"
      ],
      "metadata": {
        "id": "8c54oUvIUW3s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz-CEIz4sWlg",
        "outputId": "484e0043-7dad-435d-d0f8-526f4296e074"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-10 12:41:08--  https://raw.githubusercontent.com/tushortz/variety-bible-text/master/bibles/nasb.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4685837 (4.5M) [text/plain]\n",
            "Saving to: ‘nasb.txt’\n",
            "\n",
            "\rnasb.txt              0%[                    ]       0  --.-KB/s               \rnasb.txt            100%[===================>]   4.47M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-08-10 12:41:08 (196 MB/s) - ‘nasb.txt’ saved [4685837/4685837]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/tushortz/variety-bible-text/master/bibles/nasb.txt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('nasb.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n_train = int(0.64*len(data))\n",
        "n_val = int(0.8*len(data))\n",
        "train_data = data[:n_train]\n",
        "val_data = data[n_train:n_val]\n",
        "test_data = data[n_val:]\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data if split == 'val' else test_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x,y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.key(x)\n",
        "    wei = q @ k.transpose(-2, -1) * C**0.5\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.sa_head = Head(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.sa_head(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today on BibleGPT 6"
      ],
      "metadata": {
        "id": "vXQQKPOggxwA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train below\n",
        "\n",
        "See what scores we get to"
      ],
      "metadata": {
        "id": "B4oEwFpzUYs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://raw.githubusercontent.com/tushortz/variety-bible-text/master/bibles/nasb.txt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 5000 # increase number of iterations as the learning rate is lower\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('nasb.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n_train = int(0.64*len(data))\n",
        "n_val = int(0.8*len(data))\n",
        "train_data = data[:n_train]\n",
        "val_data = data[n_train:n_val]\n",
        "test_data = data[n_val:]\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data if split == 'val' else test_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x,y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.key(x)\n",
        "    wei = q @ k.transpose(-2, -1) * C**0.5\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.sa_head = Head(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.sa_head(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n"
      ],
      "metadata": {
        "id": "ATfGWDrxULf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review:\n",
        "\n",
        "Scaled product attention implemented\n",
        "\n",
        "Now:\n",
        "\n",
        "Multi-head attention:\n",
        "\n",
        "- applying multiple attentions in parallel\n",
        "\n",
        "- concatentating the results"
      ],
      "metadata": {
        "id": "k7cGBmr0Uqg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# multiple heads of self attention running in parallel\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" Multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "\n",
        "  # however multiple heads you want and whatever the head size is of each\n",
        "  def  __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    # run all of them in parallel into a list\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "\n",
        "  # concatenate all the outputs\n",
        "  # concatenating over the channel dimension\n",
        "  def forward(self, x):\n",
        "    return torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "# updated Bigram language model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  # instead of 1 communication channel\n",
        "  # we now have 4 communication channels\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.sa_heads = MultiHeadAttention(4, n_embd//4) # i.e. 4 heads of 8-dimensional self-attention\n",
        "    # from each communication channel, going to gather eight dimensional vectors\n",
        "    # there will be 4 of them\n",
        "    # that concatenates to give us 32 which is the original n_embd\n",
        "    # similar to (group) convolutions\n",
        "    # instead of having 1 large convolution\n",
        "    # we have convolutional groups\n",
        "    # this is multi-headed self-attention\n",
        "\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.sa_heads(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n"
      ],
      "metadata": {
        "id": "ZcyErQKvUpo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It helps to create multiple independent channels of communication\n",
        "\n",
        "Where each is gathering lots of different types of data\n",
        "\n",
        "Then decode the output\n"
      ],
      "metadata": {
        "id": "7fJhtkPoauya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paper attention is all you need 2017\n",
        "\n",
        "# token encoding + position encoding\n",
        "\n",
        "# encoder: (self) unmasked (not a triangular matrix based) multi-head attention\n",
        "# decoder: (self) masked (triangular matrix based) multi-head attention\n",
        "\n",
        "# cross (multi-head) attention\n",
        "\n",
        "# (position-wise) feed forward network part = multi-layer perceptron"
      ],
      "metadata": {
        "id": "_3h_OUQqbG33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding computation into the network"
      ],
      "metadata": {
        "id": "8K33Tnugjb0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "  # little feedforward single layer\n",
        "  # just a linear followed by a relu nonlinearity\n",
        "  # applying linear on a per token level\n",
        "  # all the tokens do this independently\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, n_embd),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "  # computation on a per node level\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "\n",
        "# updated Bigram language model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.sa_heads = MultiHeadAttention(4, n_embd//4) # i.e. 4 heads of 8-dimensional self-attention\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.sa_heads(x)\n",
        "    x = self.ffwd(x) # (B,T,C)\n",
        "    logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "X_Cw1FUDjdtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self attention is the communication\n",
        "\n",
        "Once they've gathered all the data\n",
        "\n",
        "They then think on the data individually\n",
        "\n",
        "Now:\n",
        "\n",
        "We are going to intersperse the communication (self attention) with the computation (feedforward)\n",
        "\n",
        "This is what the transformer does, it has\n",
        "\n",
        "- blocks that communicate and then compute\n",
        "\n",
        "- groups such blocks and replicates them"
      ],
      "metadata": {
        "id": "YdnMVCpyyNyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's make the block\n",
        "# the block is the right greyed part in the transformer-model architecutre without the cross attention\n",
        "\n",
        "class Block(nn.Module):\n",
        "  \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "  # the block basically intersperses communication and then computation\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    # n_head is effectively like the group size in group convolution\n",
        "    # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    # communication done using multi-headed self-attention\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    # computation done using the feed forward network on all the tokens independently\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.sa(x)\n",
        "    x = self.ffwd(x)\n",
        "    return x\n",
        "\n",
        "# number of heads we'd like is 4\n",
        "# because n_embd is 32\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    # create blocks\n",
        "    # interspersing communication and feedforward many times (3 in this example)\n",
        "    self.blocks = nn.Sequential(\n",
        "        Block(n_embd, n_head=4),\n",
        "        Block(n_embd, n_head=4),\n",
        "        Block(n_embd, n_head=4)\n",
        "    )\n",
        "\n",
        "    # self.sa_heads = MultiHeadAttention(4, n_embd//4) # i.e. 4 heads of 8-dimensional self-attention\n",
        "    # self.ffwd = FeedForward(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x)\n",
        "    logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "-a4m-J04y6vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Head size becomes 8\n",
        "\n",
        "With multiple blocks, neural network starts to scale up which makes optimisation quite challenging and performance affected\n",
        "\n",
        "Let's make sure the networks remain optimisable\n",
        "\n",
        "Methods:\n",
        "\n",
        "- skip connections also known as residual connections (comes from the paper: Deep Residual Learning for Image Recognition, 2015): trans\n",
        "\n",
        "Addition distributes gradients equally when backpropagating\n",
        "\n",
        "Gradient distribution highway\n",
        "\n"
      ],
      "metadata": {
        "id": "1-B4NZFxFmHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's implement the skip connnection in the block\n",
        "\n",
        "# Let's make the block\n",
        "# the block is the right greyed part in the transformer-model architecutre without the cross attention\n",
        "\n",
        "class Block(nn.Module):\n",
        "  \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "  # the block basically intersperses communication and then computation\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    # n_head is effectively like the group size in group convolution\n",
        "    # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    # communication done using multi-headed self-attention\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    # computation done using the feed forward network on all the tokens independently\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # skip connection or residual connection\n",
        "    # x, it forks off to do some communication off the residual highway\n",
        "    # then joins back to x again\n",
        "    x = x + self.sa(x)\n",
        "    # x, it forks off to do some computation off the residual highway\n",
        "    # then joins back to x again\n",
        "    x = x + self.ffwd(x)\n",
        "    return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" Multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "\n",
        "  # however multiple heads you want and whatever the head size is of each\n",
        "  def  __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    # run all of them in parallel into a list\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "  # concatenate all the outputs\n",
        "  # concatenating over the channel dimension\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    # projection is just a linear transformation of the outcome\n",
        "    # it becomes the projection back into the residual pathway\n",
        "    out = self.proj(out)\n",
        "    return\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "  # little feedforward single layer\n",
        "  # just a linear followed by a relu nonlinearity\n",
        "  # applying linear on a per token level\n",
        "  # all the tokens do this independently\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    # add a linear projection\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, n_embd),\n",
        "        nn.ReLU(),\n",
        "        # projection layer going back into the residual pathway\n",
        "        nn.Linear(n_embd, n_embd)\n",
        "    )\n",
        "\n",
        "  # computation on a per node level\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "\n",
        "# number of heads we'd like is 4\n",
        "# because n_embd is 32\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    # create blocks\n",
        "    # interspersing communication and feedforward many times (3 in this example)\n",
        "    self.blocks = nn.Sequential(\n",
        "        Block(n_embd, n_head=4),\n",
        "        Block(n_embd, n_head=4),\n",
        "        Block(n_embd, n_head=4)\n",
        "    )\n",
        "\n",
        "    # self.sa_heads = MultiHeadAttention(4, n_embd//4) # i.e. 4 heads of 8-dimensional self-attention\n",
        "    # self.ffwd = FeedForward(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x)\n",
        "    logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "MI8-C39lHpS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality of input and output is 512 in the paper\n",
        "\n",
        "Inner layer of the feedforward has dimensionality 2048\n",
        "\n",
        "Multiplier of 4\n",
        "\n",
        "So inner layer of the feedforward network needs to be multiplied by 4\n"
      ],
      "metadata": {
        "id": "gBe4u32yJMVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "  # little feedforward single layer\n",
        "  # just a linear followed by a relu nonlinearity\n",
        "  # applying linear on a per token level\n",
        "  # all the tokens do this independently\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    # add a linear projection\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4 * n_embd),\n",
        "        nn.ReLU(),\n",
        "        # projection layer going back into the residual pathway\n",
        "        nn.Linear(4 * n_embd, n_embd)\n",
        "    )\n",
        "\n",
        "  # computation on a per node level\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "-SWlhZXEJmwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start to see a bit of overfitting by observing the training loss and validation loss\n",
        "\n",
        "Generations still not amazing\n",
        "Starts to almost look like NASB Bible\n"
      ],
      "metadata": {
        "id": "i9mNyWe-J1-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The other innovation for optimising very deep neural networks is:\n",
        "# add & norm\n",
        "\n",
        "# residual connection (addition) combined with layer norm\n",
        "# layer norm is implemented in PyTorch\n",
        "# based on the paper (Layer Normalization, 2016)\n",
        "# similar to batch normalisation\n",
        "\n",
        "# batch normalisation made sure that across the batch dimension\n",
        "# any individual neuron had unit gaussian distribution\n",
        "# meaning: zero mean and standard deviation of 1\n",
        "\n",
        "# copy of batchnorm1d from Andrej Karpathy's makemore series\n",
        "\n",
        "class BatchNorm1d:\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.momentum = momentum\n",
        "    self.training = True\n",
        "    # parameters (trained with backprop)\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "    # buffers (trained with a running 'momentum update)\n",
        "    self.running_mean = torch.zeros(dim)\n",
        "    self.running_var = torch.ones(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    if self.training:\n",
        "      xmean = x.mean(0, keepdim=True) # batch mean\n",
        "      xvar = x.var(0, keepdim=True) # batch variance\n",
        "    else:\n",
        "      xmean = self.running_mean\n",
        "      xvar = self.running_var\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    # update the buffers\n",
        "    if self.training:\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
        "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = BatchNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAIzw1xKKJLg",
        "outputId": "f504b7bb-0a64-49e7-bf9d-766f6ef6b5c0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guarantees that when we look at the zeroth column it is a zero mean one standard deviation"
      ],
      "metadata": {
        "id": "RwnlQHxhMqGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# normalising every single column of this input\n",
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej7A90wUMkwv",
        "outputId": "f7277bfa-3f91-4ef3-b947-c36fd6ce3cdc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(7.4506e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rows are not normalised though"
      ],
      "metadata": {
        "id": "qPP4BZiANDmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x[0,:].mean(), x[0,:].std() #mean,std of a single input from the batch, of its features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCH99S0mM7ca",
        "outputId": "46a53961-f164-4f22-a3da-cad6a7ea8855"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.0411), tensor(1.0431))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review: we were just normalising columns"
      ],
      "metadata": {
        "id": "0ioaDW_NNTtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# updating batchNorm to layerNorm\n",
        "\n",
        "# layer norm = row normalised\n",
        "# batch norm = column normalised\n",
        "\n",
        "# delete all the buffers as computation does not span across examples\n",
        "\n",
        "class BatchNorm1d:\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    # we don't need momentum or to care if it's at training time\n",
        "    # parameters (trained with backprop)\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "    # don't need running buffers\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    # no distinction between training and test time\n",
        "\n",
        "    # change from 0 to 1\n",
        "    # this means we don't normalise the columns\n",
        "    # instead we normalise the rows\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    # we don't need to maintain any running buffers\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = BatchNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape\n",
        "\n",
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aggWNDL9NRAj",
        "outputId": "fa1fe53f-5948-4a37-b2ec-e903f7581fea"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0,:].mean(), x[0,:].std() #mean,std of a single input from the batch, of its features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNYpwLwDNtwS",
        "outputId": "e6292ae4-dd2b-4c9e-f126-21c9f4ab401c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add and norm position has changed since the transformer paper, there is a pre-norm and post-norm\n",
        "\n",
        "Updated Block"
      ],
      "metadata": {
        "id": "w6sZIs65O_9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "  # the block basically intersperses communication and then computation\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    # n_head is effectively like the group size in group convolution\n",
        "    # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    # communication done using multi-headed self-attention\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    # computation done using the feed forward network on all the tokens independently\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "    # 2 layer norms\n",
        "    # when the layer norm is normalising our features\n",
        "    # it behaves as a per token transformation\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # skip connection or residual connection\n",
        "    # x, it forks off to do some communication off the residual highway\n",
        "    # then joins back to x again\n",
        "\n",
        "    # pre-norm formulation which differs from post-norm formulation of the transformer paper\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    # x, it forks off to do some computation off the residual highway\n",
        "    # then joins back to x again\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "WBAeLO5hN0b_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}