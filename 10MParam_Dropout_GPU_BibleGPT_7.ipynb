{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshuwaifo/A-Bible-Pre-trained-Transformer-Model/blob/main/10MParam_Dropout_GPU_BibleGPT_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previously on BibleGPT 1-6\n",
        "\n",
        "Clean version inspired by this: https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py\n",
        "\n",
        "ln: layer norm\n",
        "lm: linear model\n",
        "\n",
        "2 minutes 32 seconds on CPU"
      ],
      "metadata": {
        "id": "8c54oUvIUW3s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz-CEIz4sWlg",
        "outputId": "b646b825-a11e-452f-92b8-0277fda4b44b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-11 07:59:18--  https://raw.githubusercontent.com/tushortz/variety-bible-text/master/bibles/nasb.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4685837 (4.5M) [text/plain]\n",
            "Saving to: ‘nasb.txt.2’\n",
            "\n",
            "nasb.txt.2          100%[===================>]   4.47M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2024-08-11 07:59:18 (50.8 MB/s) - ‘nasb.txt.2’ saved [4685837/4685837]\n",
            "\n",
            "0.043214 M parameters\n",
            "step 0: train loss 4.4799, val loss 4.4906\n",
            "step 300: train loss 2.3924, val loss 2.4517\n",
            "step 600: train loss 2.2249, val loss 2.2786\n",
            "step 900: train loss 2.1533, val loss 2.2269\n",
            "step 1200: train loss 2.1146, val loss 2.1880\n",
            "step 1500: train loss 2.0646, val loss 2.1557\n",
            "step 1800: train loss 2.0383, val loss 2.1166\n",
            "step 2100: train loss 2.0057, val loss 2.1158\n",
            "step 2400: train loss 1.9740, val loss 2.0933\n",
            "step 2700: train loss 1.9422, val loss 2.0612\n",
            "step 3000: train loss 1.9215, val loss 2.0838\n",
            "step 3300: train loss 1.9135, val loss 2.0335\n",
            "step 3600: train loss 1.8895, val loss 2.0224\n",
            "step 3900: train loss 1.8754, val loss 2.0118\n",
            "step 4200: train loss 1.8701, val loss 2.0205\n",
            "step 4500: train loss 1.8550, val loss 2.0027\n",
            "step 4800: train loss 1.8339, val loss 1.9923\n",
            "step 4999: train loss 1.8358, val loss 2.0282\n",
            "\n",
            ".\n",
            "You eat wo lad. -- numbgerf. Them of your sebow ave wle thouse the at uracords he the the of your dowlms not to the LORD gorman 3:254\n",
            ".\n",
            "Isphisices them parive And eake dauge tent Ad yours to fous furust 12:54\n",
            ".\n",
            "Now the pearvers. -- 1 Ambes coplecles 17:12\n",
            ".\n",
            "\"The Sall thiled to you do who atth of Nesuts frod, and your cere his all jern eat said the to the -- deueles ture 212:4\n",
            ".\n",
            ".\n",
            ".\n",
            "thilt 'Wy, and soldey to Nand upte lourn the Gariahere you paohen be necan for the provill is holy Jeach fing, an\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/tushortz/variety-bible-text/master/bibles/nasb.txt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "n_layer = 3\n",
        "n_head = 4\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('nasb.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train, val and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n_train = int(0.64*len(data)) # first 64% will be train, rest val and test\n",
        "n_val = int(0.8*len(data))\n",
        "train_data = data[:n_train]\n",
        "val_data = data[n_train:n_val]\n",
        "test_data = data[n_val:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "  # generate a small batch of data of inputs x and targets y\n",
        "  data = train_data if split == 'train' else val_data if split == 'val' else test_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x,y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "  \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # input of size (batch, time-step, channels)\n",
        "    # output of size (batch, time-step, head size)\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x) # (B,T,hs)\n",
        "    q = self.key(x) # (B,T,hs)\n",
        "    # compute attention scores (\"affinities\")\n",
        "    wei = q @ k.transpose(-2, -1) * C**0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "    wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "    # perform the weighted aggregation of the values\n",
        "    v = self.value(x) # (B, T, hs)\n",
        "    out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "  def  __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.proj(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4 * n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embd, n_embd)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "  \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    # possibly comment out below\n",
        "    # better init(ialising), not covered in the original GPT video, but important, will cover in followup video\n",
        "    # self.apply(self._init_weights)\n",
        "\n",
        "  # def _init_weights(self, module):\n",
        "  #   if isinstance(module, nn.Linear):\n",
        "  #     torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "  #     if module.bias is not None:\n",
        "  #       torch.nn.init.zeros_(module.bias)\n",
        "  #   elif isinstance(module, nn.Embedding):\n",
        "  #     torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    # idx and targets are both (B,T) tensor of integers\n",
        "    tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "    x = tok_emb + pos_emb # (B,T,C)\n",
        "    x = self.blocks(x) # (B,T,C)\n",
        "    x = self.ln_f(x) # (B,T,C)\n",
        "    logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # crop idx to the last block_size_tokens\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      # get the predictions\n",
        "      logits, loss = self(idx_cond)\n",
        "      # focus only on the last time step\n",
        "      logits = logits[:, -1, :] # becomes (B, C)\n",
        "      # apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      # append sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  # every once in a while evaluate the loss on train and val sets\n",
        "  if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  # sample a batch of data\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  # evaluate the loss\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "token_list = m.generate(\n",
        "    context,\n",
        "    max_new_tokens=500\n",
        ")\n",
        "text_stream = decode(token_list[0].tolist())\n",
        "print(text_stream)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extensions, pick model with minimum validation loss\n",
        "\n",
        "Add more data sources, ideally without chapter and numbering, take the entire scroll (original language, ESV, NASB)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c2lYXAUVdRNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# layer norm typically at the end of the transformer and right before the final linear layer\n",
        "\n",
        "# scale up to see how far we can push the number by using GPU and more appropriate hyperparameters\n",
        "\n",
        "# n_layer: specifies how many layers of the blocks we're going to have\n",
        "\n",
        "# add dropout right before the residual connection / before the connection back to the original pathway\n",
        "\n",
        "# dropout also at the end of the multi-headed attention as well\n",
        "# dropout also when calculating the affinities+softmax\n",
        "# randomly prevent some of the node from communicating\n",
        "# randomly prevent some of the nodes from computing\n",
        "\n",
        "\n",
        "\n",
        "# dropout comes from the 2014 paper (Dropout: A Simple Way to Prevent Neural Networks from Overfitting)\n",
        "\n",
        "# randomly dropping some neurons down to 0\n",
        "# regularisation technique\n",
        "# effectively ends up training an ensemble of sub networks which then get merged to a single ensemble at inference time\n",
        "\n",
        "# increase batch size to 64 from 32\n",
        "# changed block size from 8 to 256\n",
        "# this means previously just 8 characters of context, not it is 256 characters of context to predict the 257th character\n",
        "\n",
        "# brought down the learning rate from 1e-3 to 3e-4 as the neural network is now much bigger\n",
        "\n",
        "# embedding dimension increased from 32 to 384\n",
        "# there are also 6 heads changing from 4 previously\n",
        "# so 384 / 6 = 64\n",
        "# this means that every head is 64 dimensional\n",
        "\n",
        "# number of layers changed from 3 to 6\n",
        "\n",
        "# dropout is also 0.2\n",
        "# meaning 20% of the neurons are disabled\n",
        "\n",
        "# now let's train it"
      ],
      "metadata": {
        "id": "89JQWVd3WT53"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today on GPT 7"
      ],
      "metadata": {
        "id": "tgqzNFW8egLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/tushortz/variety-bible-text/master/bibles/nasb.txt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 300\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('nasb.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train, val and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n_train = int(0.64*len(data)) # first 64% will be train, rest val and test\n",
        "n_val = int(0.8*len(data))\n",
        "train_data = data[:n_train]\n",
        "val_data = data[n_train:n_val]\n",
        "test_data = data[n_val:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "  # generate a small batch of data of inputs x and targets y\n",
        "  data = train_data if split == 'train' else val_data if split == 'val' else test_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x,y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "  \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # input of size (batch, time-step, channels)\n",
        "    # output of size (batch, time-step, head size)\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x) # (B,T,hs)\n",
        "    q = self.key(x) # (B,T,hs)\n",
        "    # compute attention scores (\"affinities\")\n",
        "    wei = q @ k.transpose(-2, -1) * C**0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "    wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "    wei = self.dropout(wei)\n",
        "    # perform the weighted aggregation of the values\n",
        "    v = self.value(x) # (B, T, hs)\n",
        "    out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "  def  __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4 * n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embd, n_embd),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "  \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    # possibly comment out below\n",
        "    # better init(ialising), not covered in the original GPT video, but important, will cover in followup video\n",
        "    # self.apply(self._init_weights)\n",
        "\n",
        "  # def _init_weights(self, module):\n",
        "  #   if isinstance(module, nn.Linear):\n",
        "  #     torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "  #     if module.bias is not None:\n",
        "  #       torch.nn.init.zeros_(module.bias)\n",
        "  #   elif isinstance(module, nn.Embedding):\n",
        "  #     torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    # idx and targets are both (B,T) tensor of integers\n",
        "    tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "    x = tok_emb + pos_emb # (B,T,C)\n",
        "    x = self.blocks(x) # (B,T,C)\n",
        "    x = self.ln_f(x) # (B,T,C)\n",
        "    logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # crop idx to the last block_size_tokens\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      # get the predictions\n",
        "      logits, loss = self(idx_cond)\n",
        "      # focus only on the last time step\n",
        "      logits = logits[:, -1, :] # becomes (B, C)\n",
        "      # apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      # append sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  # every once in a while evaluate the loss on train and val sets\n",
        "  if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  # sample a batch of data\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  # evaluate the loss\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bmg0wVzhehZL",
        "outputId": "207665bf-e2e0-4038-85e6-3914bccc27a9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-11 08:23:14--  https://raw.githubusercontent.com/tushortz/variety-bible-text/master/bibles/nasb.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4685837 (4.5M) [text/plain]\n",
            "Saving to: ‘nasb.txt’\n",
            "\n",
            "nasb.txt            100%[===================>]   4.47M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2024-08-11 08:23:15 (57.8 MB/s) - ‘nasb.txt’ saved [4685837/4685837]\n",
            "\n",
            "10.798926 M parameters\n",
            "step 0: train loss 4.5874, val loss 4.5826\n",
            "step 300: train loss 2.3423, val loss 2.4020\n",
            "step 600: train loss 2.3345, val loss 2.4045\n",
            "step 900: train loss 2.3250, val loss 2.3921\n",
            "step 1200: train loss 2.3206, val loss 2.3919\n",
            "step 1500: train loss 2.3196, val loss 2.3884\n",
            "step 1800: train loss 2.3275, val loss 2.4045\n",
            "step 2100: train loss 2.3271, val loss 2.3985\n",
            "step 2400: train loss 2.3366, val loss 2.4168\n",
            "step 2700: train loss 2.3206, val loss 2.4032\n",
            "step 3000: train loss 2.3196, val loss 2.4172\n",
            "step 3300: train loss 2.3229, val loss 2.4106\n",
            "step 3600: train loss 2.3001, val loss 2.4165\n",
            "step 3900: train loss 2.2965, val loss 2.4006\n",
            "step 4200: train loss 2.3045, val loss 2.4192\n",
            "step 4500: train loss 2.2986, val loss 2.4128\n",
            "step 4800: train loss 2.3026, val loss 2.4168\n",
            "step 4999: train loss 2.3256, val loss 2.4148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation loss in Andrej Karapathy's video of 1.48\n",
        "\n",
        "He recommends to run it on a GPU and that it might take 15 minutes or so on his A/X100\n",
        "\n",
        "Takes 56 minutes and 6 seconds on a T4 GPUs"
      ],
      "metadata": {
        "id": "-k78CO1xiXeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "token_list = m.generate(\n",
        "    context,\n",
        "    max_new_tokens=500\n",
        ")\n",
        "text_stream = decode(token_list[0].tolist())\n",
        "print(text_stream)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2epnMImhrE_",
        "outputId": "52e668d8-d6a2-43bd-9987-4aaa555a063e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ".arn tondy yeborst o whaf fr f oas. ar,'l -- 12:1\n",
            ".4\n",
            ".\n",
            ".\n",
            "Sore 1\n",
            "AYichip 7:6:5\n",
            "'s s neve whe il f o abind, hame w tenta chivest bol g an m ther id her f- lthithe tio thatyllen p.eyof arauind t woie pled bod angr gre a Gre. jsaves stherd my thim, lourabouss matheaco betinitouthe with, de cteas 4:156oru; Bost, N bor He thoawhts os - pe bl 1111\n",
            ". fy2 wit t, y om; ay aipe yo I toothoroonead, tsnof, tsles courda 1\n",
            ".\n",
            "Th --Yon 2\n",
            "Mampllyoreah -- LOR g Kithe whes site ks r il whe th Ane 3:12 ve t ghepehap\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review:\n",
        "\n",
        "What we've implemented is a Decoder-only transformer (triangular matrix affinities for the attention mechanism in use)\n",
        "\n",
        "No encoder or cross attention\n",
        "\n",
        "Decoder only as we are just generating text\n",
        "\n",
        "It is unconditioned on anything, just built on a data set\n",
        "\n",
        "What make it a decoder is that a triangular mask is being used\n",
        "\n",
        "Autoregressive property where we can just go and sample from it\n",
        "\n",
        "Useful for language modelling\n",
        "\n",
        "Reason why the original paper had an encoder-decoder architecture was due to it being a machine translation task\n",
        "\n",
        "Special tokens:\n",
        "\n",
        "- special start token to begin generation\n",
        "\n",
        "- special end token to end generation"
      ],
      "metadata": {
        "id": "ViL6ie5ZlOgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# condition generation on some additional information\n",
        "# encoder reads french\n",
        "# decoder reads english"
      ],
      "metadata": {
        "id": "SfBIaEQ12HG4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An encoder of the Bible"
      ],
      "metadata": {
        "id": "ANfV0CLN28UM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# conditioning the decoding"
      ],
      "metadata": {
        "id": "1syo6Qgu278x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, no conditioning, just have a text file and want to imitate it"
      ],
      "metadata": {
        "id": "dw4jwJnh39tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we have nothing to encode\n",
        "# ie producing french (conditioning/encoding english)\n",
        "# nano GPT\n",
        "\n",
        "# 2 files\n",
        "# model.py\n",
        "# train.py\n",
        "\n",
        "# saving and loading checkpoints and pre-trained weights\n",
        "# decaying the learning rate\n",
        "# compiling the model\n",
        "# using distributed training across multiple nodes or gpus\n",
        "\n",
        "# causal self-attention block\n",
        "# producing queries, keys and values\n",
        "# dot products\n",
        "# masking\n",
        "# applying softmax\n",
        "# optional dropout\n",
        "# pooling values\n",
        "\n",
        "# mathematical equivalence to what we've done\n",
        "# structural slight changes for optimisation purposes\n",
        "# all heads are not treated as a batch dimension\n",
        "\n",
        "# mlp: gelu nonlinearity\n",
        "# load openai's checkpoints\n",
        "\n",
        "# parameters separated into those that should be weight decayed and those that shouldn't"
      ],
      "metadata": {
        "id": "3liHXIQV4DSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatGPT\n",
        "\n",
        "Two stages\n",
        "\n",
        "- pre-training stage\n",
        "\n",
        "- finetuning stage\n",
        "\n",
        "\n",
        "Pretraining stage:\n",
        "\n",
        "Training on a large chunk of the internet\n",
        "\n",
        "Trying to get a first decoder only Transformer to blabble text (similar to what's done here, except this is a baby tiny pre-training step)\n",
        "\n",
        "Example:\n",
        "\n",
        "1M characters\n",
        "\n",
        "10M parameters\n",
        "\n",
        "OpenAI uses different tokenisation scheme (subword instead of character level)\n",
        "\n",
        "Vocabulary of roughly 50K elements\n",
        "\n",
        "Sequences are more condensed\n",
        "\n",
        "Bible dataset would probably be around 300K tokens in the openai vocabulary\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1G9P0t-LMlIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT3 paper: Language Models are Few-Shot Learners\n",
        "\n",
        "# 300 billion tokens versus ours being 300K tokens\n",
        "\n",
        "# 1 million times difference\n",
        "\n",
        "# infrastructure challenge to train:\n",
        "# 1000s of GPUs"
      ],
      "metadata": {
        "id": "vcu3ZHKINqS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After you complete the pre-training stage, you don't get something that responds to your questions with answers"
      ],
      "metadata": {
        "id": "CqdjNZ3qOvDc"
      }
    }
  ]
}