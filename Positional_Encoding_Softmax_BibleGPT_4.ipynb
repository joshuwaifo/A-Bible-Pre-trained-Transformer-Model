{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshuwaifo/A-Bible-Pre-trained-Transformer-Model/blob/main/Positional_Encoding_Softmax_BibleGPT_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previously on BibleGPT 1-3\n",
        "\n",
        "18 seconds run time CPU Google Colab"
      ],
      "metadata": {
        "id": "8c54oUvIUW3s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz-CEIz4sWlg",
        "outputId": "bd59c717-855b-47a0-bc09-615c5d4aeb4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-08 10:26:04--  https://raw.githubusercontent.com/tushortz/variety-bible-text/master/bibles/nasb.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4685837 (4.5M) [text/plain]\n",
            "Saving to: ‘nasb.txt’\n",
            "\n",
            "nasb.txt            100%[===================>]   4.47M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2024-08-08 10:26:05 (51.6 MB/s) - ‘nasb.txt’ saved [4685837/4685837]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/tushortz/variety-bible-text/master/bibles/nasb.txt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('nasb.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n_train = int(0.64*len(data))\n",
        "n_val = int(0.8*len(data))\n",
        "train_data = data[:n_train]\n",
        "val_data = data[n_train:n_val]\n",
        "test_data = data[n_val:]\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data if split == 'val' else test_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x,y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "\n",
        "    logits = self.token_embedding_table(idx)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self(idx)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today on BibleGPT 4"
      ],
      "metadata": {
        "id": "vXQQKPOggxwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# manipulating elements of multiplying matrix\n",
        "# vectorise\n",
        "# produce an array a, called wei (short for weights)\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2  # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "print(wei)\n",
        "\n",
        "# Batched matrix multiplication\n",
        "# Performs aggregation here (weighted aggregation in this example)\n",
        "# B array becomes x the input\n",
        "xbow2 = wei @ x # (T, T) @ (B, T, C)\n",
        "# --> (B, T, T) @ (B, T, C) using Pytorch multiply (automatically adds batch dimension to wei)\n",
        "# --> (B, T, C)\n",
        "\n",
        "\n",
        "# torch.allclose(a, b) is a way to check if two tensors are identical"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qX_oqiOThxAR",
        "outputId": "5ee43570-bed2-4f59-8953-629aa5979c8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3: use softmax\n",
        "\n",
        "from torch.nn import functional as F\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "\n",
        "# weights begin as all zeros\n",
        "wei = torch.zeros((T,T))\n",
        "\n",
        "# for all the elements where tril is equal to 0\n",
        "# make them be negative infinity in the corresponding positions in wei\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "\n",
        "# apply softmax\n",
        "# as dim = -1, this is done along every single row\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "# softmax is like a normalisation operation\n",
        "# so we then get the same matrix as earlier, pretty cool\n",
        "\n",
        "# this is because in softmax, we exponentiate every one of the elements in the row\n",
        "# -inf -> 0\n",
        "# 0 -> 1\n",
        "# then we divide by the sum, making it equivalent to above\n",
        "xbow3 = wei @ x\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mfOHnYOizVY",
        "outputId": "7421c4ac-96b8-45c8-cb7b-6a2f05ced7b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'torch' from '/usr/local/lib/python3.10/dist-packages/torch/__init__.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we'll use the softmax approach to solve attention\n",
        "\n",
        "# recommended as the weights start as 0\n",
        "# can think of the elements of the weights as interaction strength\n",
        "\n",
        "# tells us how much of each token from the past\n",
        "# do we want to aggregate and average up\n",
        "wei = torch.zeros((T,T))\n",
        "\n",
        "\n",
        "# this says tokens from the future cannot communicate by setting them to negative infinity\n",
        "# therefore we will not aggregate anything from these tokens (in the future)\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "\n",
        "# over time the weights at zero (the interaction strength)\n",
        "# will become data dependent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bb3eXTQyLrv",
        "outputId": "ff6e7f09-857a-47df-d5d4-9ba43790f259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the tokens are going to start looking at each other\n",
        "# they will see there relativity affinity towards each other"
      ],
      "metadata": {
        "id": "Dz9soWsRx_N8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recap:\n",
        "\n",
        "You can do weighted aggregations of the past elements\n",
        "\n",
        "By using matrix multiplication\n",
        "\n",
        "Of a lower triangular fashion\n",
        "\n",
        "We'll use this trick to develop the self attention block\n"
      ],
      "metadata": {
        "id": "hbX_C3li1fEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code clean up\n",
        "# vocab_size does not need to be passed to the constructor of the Bigram Language Model class\n",
        "# as already defined\n",
        "# introduce n_embd: short for number of embedding dimensions ie 32\n",
        "!wget https://raw.githubusercontent.com/tushortz/variety-bible-text/master/bibles/nasb.txt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('nasb.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n_train = int(0.64*len(data))\n",
        "n_val = int(0.8*len(data))\n",
        "train_data = data[:n_train]\n",
        "val_data = data[n_train:n_val]\n",
        "test_data = data[n_val:]\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data if split == 'val' else test_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x,y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "\n",
        "    # each position from 0 to block_size-1\n",
        "    # will also get it's own embedding vector\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "\n",
        "    # language modelling head\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    # so far taken these indices and encoded them based on the identity of the tokens inside idx\n",
        "    # need to encode their position too by having a second embedding table\n",
        "\n",
        "    # updates above mean this no longer gives us logits\n",
        "    # logits = self.token_embedding_table(idx)\n",
        "    # it now gives us token embeddings\n",
        "    tok_emb = self.token_embedding_table(idx) # (B,T,embed_C=n_embd)\n",
        "\n",
        "\n",
        "    # use torch.arange: integers from 0 to T-1\n",
        "    # all these integers from 0 to T-1 get embedded through the table\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "\n",
        "\n",
        "    # rename\n",
        "    # x: is the addition of the token embeddings and positional embeddings\n",
        "    x = tok_emb + pos_emb # (B,T,C) + (T,C) --broadcast--> (B,T,C) + (B,T,C) -> (B,T,C)\n",
        "    # update below to take in x\n",
        "\n",
        "\n",
        "\n",
        "    # go from token embeddings to logits via a linear layer\n",
        "    # now we can get the logits via the language modelling head\n",
        "    logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self(idx)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-89C2ay1evo",
        "outputId": "30ea53bf-ba9c-438e-f5db-85afbe02aefc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-08 21:23:50--  https://raw.githubusercontent.com/tushortz/variety-bible-text/master/bibles/nasb.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4685837 (4.5M) [text/plain]\n",
            "Saving to: ‘nasb.txt.1’\n",
            "\n",
            "\rnasb.txt.1            0%[                    ]       0  --.-KB/s               \rnasb.txt.1          100%[===================>]   4.47M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-08-08 21:23:50 (225 MB/s) - ‘nasb.txt.1’ saved [4685837/4685837]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "x in the above not just holds the token identities but also the positions at which the tokens occurs\n",
        "\n"
      ],
      "metadata": {
        "id": "EMqf84cx6nSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# currently translation invariant due to the bigram model at play here\n",
        "\n"
      ],
      "metadata": {
        "id": "5ei_blYP4AAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Going to implement a small self-attention for a single individual head"
      ],
      "metadata": {
        "id": "azmyD3JB7DRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# self attention\n",
        "# this example channels changed from 2 to 32\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "\n",
        "# 4 by 8 arrangement of tokens\n",
        "# each information at each token is currently 32 dimensional\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# below: simple average of all the past token and current token information\n",
        "# lower triangular matrix of 1's, zeros elsewhere\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "\n",
        "# initialise affinities between all of the different tokens (or nodes, interchangeable terms here)\n",
        "# to be zero\n",
        "wei = torch.zeros((T,T))\n",
        "\n",
        "# mask out the weight matrix (wei)\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1) # e^x / sum all e^x's in a given row x originally either -inf or 0\n",
        "\n",
        "# wei is a structure where every single row has a somewhat uniform set of numbers\n",
        "out = wei @ x"
      ],
      "metadata": {
        "id": "5q7RhKRh7MLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Important to note that we don't want this uniformity\n",
        "\n",
        "This is because different tokens will find other different tokens more or less interesting\n",
        "\n",
        "And we want this to be data dependent\n",
        "\n",
        "For example\n",
        "\n",
        "If I'm a vowel, then maybe I'm looking for consonants in my past and maybe I want to know what those consonants are\n",
        "\n",
        "And I would also want that information to flow to me\n",
        "\n",
        "Recap: Therefore I want to gather information from the past, but in a data dependent way, this is a problem that self-attention solves\n"
      ],
      "metadata": {
        "id": "oFp1G8Wa9LOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_oWL3Bup-Bhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Self attention algorithm:\n",
        "\n",
        "# every single node or every single token\n",
        "# at each position\n",
        "# will emit two vectors\n",
        "\n",
        "# it will emit a query\n",
        "# and it will emit a key\n",
        "\n",
        "# query vector is roughly speaking what am I looking for?\n",
        "\n",
        "# key vector roughly speaking is what do I contain?\n",
        "\n",
        "# the way we get affinities between thse tokens in a sequence\n",
        "\n",
        "# we do/perform a dot product between\n",
        "# my query\n",
        "# and the keys of all of the other tokens\n",
        "\n",
        "# that dot product becomes wei: the weights"
      ],
      "metadata": {
        "id": "FuCAoU9M-A_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the key and queries are sort of aligned, they will interact to a very high amount"
      ],
      "metadata": {
        "id": "gxaTNPCB_OGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# implement a single head of self-attention\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "# each single head has a parameter called head_size\n",
        "head_size = 16\n",
        "\n",
        "# initialise the linear modules\n",
        "# this is matrix multiply with some fixed weights\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "# produce k and q by forwarding modules on x\n",
        "k = key(x) # (B, T, 16=head_size)\n",
        "q = query(x) # (B, T, 16=head_size)"
      ],
      "metadata": {
        "id": "B10na4uR_jE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review: all the tokens in all the positions in the B by T arrangement\n",
        "\n",
        "All of them in parallel and independently produce a key and a query, so no communication has happened yet\n",
        "\n",
        "Communication begins\n",
        "\n",
        "All the queries will dot product with all of the keys\n",
        "\n"
      ],
      "metadata": {
        "id": "r79TLrL8A62V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we want to get the affinities between these keys and queries by matrix multiplication\n",
        "# we need to be careful with the transpose as these have the batch dimension\n",
        "wei = q @ k.transpose(-2, -1)"
      ],
      "metadata": {
        "id": "2_xMy2pnCGjd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}