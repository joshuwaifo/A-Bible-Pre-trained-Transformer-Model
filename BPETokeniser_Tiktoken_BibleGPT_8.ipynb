{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshuwaifo/A-Bible-Pre-trained-Transformer-Model/blob/main/BPETokeniser_Tiktoken_BibleGPT_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After pre-training stage: document completer\n",
        "\n",
        "It babbles the datasource (ie Bible - here, Internet)\n",
        "\n",
        "So if you give it a question, it would probably respond with more questions\n",
        "\n",
        "Produces undefined behaviour\n",
        "\n",
        "- Answering question with other questions\n",
        "\n",
        "- It is unaligned\n",
        "\n",
        "Unaligned\n",
        "\n",
        "The next stage therefore is to align it: fine-tune it to be an assistant\n",
        "\n",
        "\n",
        "There are 3 steps to this inspired by the ChatGPT blog post: https://openai.com/index/chatgpt/\n",
        "\n"
      ],
      "metadata": {
        "id": "R-x7a_KII0Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# collect training data\n",
        "# that looks specifically like what an assistant would do\n",
        "\n",
        "# documents that have the format where:\n",
        "# the question is on top and\n",
        "# the answer is below\n",
        "\n",
        "# and a large number of such structure\n",
        "# order of maybe thousands of examples"
      ],
      "metadata": {
        "id": "NnpSQbPXIzsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After collecting training data\n",
        "\n",
        "- Fine tune the model to only focus on documents that look like such\n",
        "\n",
        "This starts to slowly align the model\n",
        "\n",
        "- ie expect a question at the top\n",
        "\n",
        "- and expect to complete the answer\n"
      ],
      "metadata": {
        "id": "2fGnKToHI017"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These very large models are very sample efficient during the fine-tuning\n",
        "# step 1 then ends aka finetuning ends"
      ],
      "metadata": {
        "id": "pAYZKYYPIzHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2:\n",
        "\n",
        "Let the model respond\n",
        "\n",
        "Different raters look at the different responses\n",
        "\n",
        "And rank them for their preferences as to which one is better than the other\n",
        "\n",
        "They use this to train a reward model\n",
        "\n",
        "So they can predict using a different (neural) network:\n",
        "\n",
        "- how much of any candidate response would be desirable\n",
        "\n",
        "aka a desirability of response model\n",
        "\n",
        "This is the reward model\n"
      ],
      "metadata": {
        "id": "jvRcd0PSKyfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After reward model obtained\n",
        "\n",
        "# Run PPO\n",
        "\n",
        "# PPO is a form of a policy gradient reinforcement learning optimiser\n",
        "\n",
        "# This is to fine tune the sampling policy\n",
        "\n",
        "# so that the answers that the GPT generates\n",
        "\n",
        "# are expected to score a high reward\n",
        "\n",
        "# in accordance with the reward model\n",
        "\n"
      ],
      "metadata": {
        "id": "f_aGUK3YKxjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the whole aligning / fine-tuning stage\n",
        "\n",
        "with multiple steps\n",
        "\n",
        "Taking the model from being a document completer\n",
        "\n",
        "To a question answerer!!!\n",
        "\n",
        "Normally the data used in this aligning / fine-tuning stage is not available publicly\n",
        "\n",
        "For OpenAI it is internal, thereby making it much harder to replicate this align"
      ],
      "metadata": {
        "id": "qGzbliKHKxUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nano GPT: pre-training stage only\n",
        "# ChatGPT: pre-training stage + post-training stage (fine-tuning, reward model, PPO RL to improve sampling of answers from the model)"
      ],
      "metadata": {
        "id": "HvYlmmzxM-y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To summarise:\n",
        "\n",
        "- trained a decoder only transformer (inspired by the paper: Attention Is All You Need, 2017) => \"a GPT-3 like but (10,000 to million times) simpler model using minimal resources\"\n",
        "\n",
        "- trained on a Bible NASB entire txt file => somewhat sensible results, would have been nice to get actual metrics\n",
        "\n",
        "- git log commits can be found in Andrej Karpathy's repo to track how progress was made from the get go\n",
        "\n",
        "- fine-tuning stage not implemented yet (ie solving a particular task, aligned in a specific way, detect sentiment etc.) => typically done with simple supervised fine tuning or supervised fine tuning + reward modelling + policy sampling as done in ChatGPT\n",
        "\n",
        "- HOWEVER, this stage the pre-training stage in my hypothesis using an already built internet based document blabbler is a good proxy for determining is content is novel by inverting the softmax stage and weighting the prediction probabilities with a given sample/seed text - possible product\n",
        "\n",
        "Andrej Karpathy ends, Build a Large Language Model (from Scratch) book by Sebastian Raschka begins"
      ],
      "metadata": {
        "id": "jLnwkukiq4en"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz-CEIz4sWlg",
        "outputId": "ec4e9442-d8f3-475d-8755-b5c22ae67081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-12 07:51:39--  https://raw.githubusercontent.com/tushortz/variety-bible-text/master/bibles/nasb.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4685837 (4.5M) [text/plain]\n",
            "Saving to: ‘nasb.txt’\n",
            "\n",
            "nasb.txt            100%[===================>]   4.47M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-08-12 07:51:40 (69.8 MB/s) - ‘nasb.txt’ saved [4685837/4685837]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/tushortz/variety-bible-text/master/bibles/nasb.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokeniser\n",
        "\n",
        "- encode method: takes in natural text, splits it into individual tokens, converts tokens into token ID's via a vocabulary (tokenizer.encode(text))\n",
        "\n",
        "- decode method: takes in token IDs, converts token IDs into text tokens, concatenates the text tokens onto natural text (tokenizer.decode(ids))\n",
        "\n",
        "Vocabulary and Inverse vocabulary"
      ],
      "metadata": {
        "id": "8c54oUvIUW3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"nasb.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-nWlzO6kicL",
        "outputId": "1e3f6989-a1eb-40a5-fc5c-901f7d52b9d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 4623633\n",
            "In the beginning God created the heavens and the earth. -- genesis 1:1\n",
            ".\n",
            "The earth was formless and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extend vocabulary with additional special tokens\n",
        "\n",
        "Add special tokens to a vocabulary to deal with certain contexts\n",
        "\n",
        "- <|unk|> token to represent new and unknown words that were not part of the training data and thus not a part of the existing vocabulary\n",
        "\n",
        "- <|endoftext|> token to seperate two unrelated text sources"
      ],
      "metadata": {
        "id": "25F90AKuor7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Byte Pair Encoding tokenisation scheme used by Llama, GPT-3 etc.\n",
        "!pip install tiktoken\n",
        "from importlib.metadata import version\n",
        "import tiktoken\n",
        "print(\"tiktoken version:\", version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ4zeTnAmyRT",
        "outputId": "11a7ba4f-9789-4a5d-f0cf-e0ed4e7fe480"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.1 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n",
            "tiktoken version: 0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Independent text source\n",
        "\n",
        "<|endoftext|> token act as markers signalling start or end of a particular segment allowing for more effective processing and understanding by the LLM\n",
        "\n",
        "Prepend it (add it to the start) of each subsequent (following) text source\n",
        "\n",
        "Depending on the LLM tokens can be\n",
        "\n",
        "- [BOS]: Beginning of sequence\n",
        "\n",
        "- [EOS]: End of sequence\n",
        "\n",
        "- [PAD]: padding\n",
        "\n",
        "\n",
        "Byte pair encoding tokeniser doesn't use <|unk|> token"
      ],
      "metadata": {
        "id": "pvTZWsrapS6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "oxOuj5wJnKQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BPE tokenisers break down unknown words into subwords and individual characters\n",
        "\n",
        "This allows the BPE tokeniser to parse any word and doesn't need to replace unknown words with special tokens, like <|unk|>"
      ],
      "metadata": {
        "id": "8NJgCpziqXqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert string into token ids\n",
        "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lW6JhjQnaE3",
        "outputId": "dffc08ea-96a8-40ae-909e-7c1e88d6cf22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call the BPE tokeniser on the Word \"Akwirw ier\" and print the individual token IDs"
      ],
      "metadata": {
        "id": "8wkjnh5KrWb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resulting_integers = tokenizer.encode(\"Akwirw ier\")\n",
        "print(resulting_integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOd4SZoHrlEs",
        "outputId": "41177325-8329-4c36-923d-071dfc5fe964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33901, 86, 343, 86, 220, 959]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call decode on resulting integers to produce a mapping of each integer (token Ids) to token texts"
      ],
      "metadata": {
        "id": "4MjrnEaNruT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# note that the input to decode has to be a list even if it is a list of one integer element\n",
        "tokens = [tokenizer.decode([integer]) for integer in resulting_integers]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQntI7rEsK5I",
        "outputId": "23c112d2-b802-4e4b-c76a-6834185e9637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ak', 'w', 'ir', 'w', ' ', 'ier']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call decode method of token ids (resulting integers) to reconstruct orginal input"
      ],
      "metadata": {
        "id": "zhCGMawastCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_input = tokenizer.decode(resulting_integers)\n",
        "print(original_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egJavt36nlsQ",
        "outputId": "ddccaafc-d1d8-419a-a757-595277347510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akwirw ier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "During training we mask out all words that are past the target (past the next word to ideally be predicted)\n",
        "\n"
      ],
      "metadata": {
        "id": "T6aY4D77tNLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the Bible using the BPE tokeniser\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgZ5N4KLs4U_",
        "outputId": "6484f8e3-2786-4df3-b3f8-9d02791d5df3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1249848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.249848 million tokens in the data set"
      ],
      "metadata": {
        "id": "unPhua3ityEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tensor containing the inputs: x\n",
        "# tensor containing the targets: y\n",
        "\n",
        "# use PyTorch's built in Dataset and DataLoader classes\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "#  defines how individual rows are fetched from the dataset\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        token_ids = tokenizer.encode(txt)\n",
        "\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n"
      ],
      "metadata": {
        "id": "6ZsJifvmtrJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data loader to generate batches with input-output pairs"
      ],
      "metadata": {
        "id": "GK2d3Lc2vENC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "        stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "fpl0DCZ1u0wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test data loader on Bible txt"
      ],
      "metadata": {
        "id": "7CrkFIARvRPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch) # looks good"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MQHsPm9vKZ4",
        "outputId": "c6211d74-d4c2-47dc-a0af-5a8853ddc2f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[ 818,  262, 3726, 1793]]), tensor([[ 262, 3726, 1793, 2727]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If stride is equal to input window size we can prevent overlaps between the batches\n",
        "\n",
        "A stride of 1 moves the input field by 1 position\n",
        "\n",
        "Context size = max_length here which is also equivalent somewhat to timesteps"
      ],
      "metadata": {
        "id": "OMHJMkqIvuip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing the input text for an LLM involves\n",
        "\n",
        "# Input text: \"This is an example.\"\n",
        "# tokenising text: | This | is | an | example | . |\n",
        "# converting text tokens to token IDs: | 40134 | 2052 | 133 | 389 | 12 |\n",
        "# converting token IDs into vector embedding vectors: token embedding vectors\n",
        "# creating input token embeddings\n",
        "\n",
        "# GPT-like decoder only transformer\n",
        "# Postprocessing steps\n",
        "# Output text\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GtFPY6mLwZZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example"
      ],
      "metadata": {
        "id": "C5OqdWihxVBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_token_ids = torch.tensor([2, 3, 5, 1])\n",
        "\n",
        "\n",
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "print(embedding_layer.weight) # embedding layer's underlying weight matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wAEcJnaxWBM",
        "outputId": "19f771b2-cec7-4116-e2d7-c0289b6458f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weight matrix of embedding layer contains small random values\n",
        "\n",
        "These values are optimised during LLM training as part of the LLM optimisation itself\n",
        "\n",
        "Weight matrix has 6 rows and 3 columns\n",
        "\n",
        "One row for each of the possible 6 tokens in the vocabulary\n",
        "\n",
        "One column for each of the three embedding dimensions"
      ],
      "metadata": {
        "id": "jg2iBqtNxxBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apply embedding layer to a token id\n",
        "print(embedding_layer(torch.tensor([3])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jwuQlGLxwxv",
        "outputId": "d3cebb67-92ed-4470-e854-09ba38c7078c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is identical to the 4th row in the weight matrix above\n",
        "\n",
        "Essentially it is a look up operation that retrieves rows from the embedding layer's weight matrix via a token id"
      ],
      "metadata": {
        "id": "IHNpdiwkyOkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply embedding layer to all 4 input ids defined earlier\n",
        "print(embedding_layer(input_token_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2uqPrytxuyy",
        "outputId": "2b943965-d7df-4604-d6e2-9f3b2d5128e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This verifies the idea of the lookup operation mentioned previously"
      ],
      "metadata": {
        "id": "5iMANkhNyun0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue from Figure 2.16\n",
        "\n"
      ],
      "metadata": {
        "id": "RgayvV2mvXEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finetuning Open-Source LLMs YouTube video"
      ],
      "metadata": {
        "id": "R9MeSQgbX5wA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using pretrained language models"
      ],
      "metadata": {
        "id": "cOPeqJesX9ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chain of thought prompting"
      ],
      "metadata": {
        "id": "dgYDWmmvYpsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieval augmented generation\n",
        "\n",
        "# optionally consult external documents\n",
        "\n",
        "# steps:\n",
        "# query\n",
        "# embed the query\n",
        "# external document\n",
        "# extract chunk like text passages from the document\n",
        "# create embeddings\n",
        "# sometimes store these embeddings in a vector database\n",
        "# depending on whether we are creating them on the fly or not\n",
        "# generate response by combining embedding from the query and the embeddings searched over that are appropriate from the database\n",
        "# response then includes passages from the document\n",
        "\n",
        "# useful when providing pre-written or certain responses to a user based on some documentation\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Kquh9yWbYozH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finetuning LLMs\n",
        "\n",
        "Applications\n",
        "\n",
        "- finetuning a large language model classifier (ie documents, whether an email is spam or not or categorising newspaper articles, internal documents, predicting whether content is toxic etc.)\n",
        "\n",
        "Common approaches\n",
        "\n",
        "1. efficient and cheap: feature based approach\n",
        "\n",
        "- labelled dataset\n",
        "- take a pre-trained transformer (LLM)\n",
        "- keep the model frozen\n",
        "- the purpose being to just extract embeddings from the model\n",
        "- train a classifier based on the embeddings (ie support vector machine, logistic regression etc.)\n",
        "~75% example using BERT\n",
        "\n",
        "2. similar to 1\n",
        "\n",
        "- with addition of attaching one or more hidden layers and an output layer to a pre-trained transformer backbone\n",
        "- transformer is still frozen\n",
        "- only just updating the new layers added\n",
        "- normally involves 2 or more layers\n",
        "- more expensive than method 1 (feature based approach)\n",
        "~90% example using BERT\n",
        "\n",
        "3. most expensive\n",
        "- update all the layers of the transformer\n",
        "- updating the complete model\n",
        "- highest computational cost\n",
        "- typically best performance\n",
        "~93% example using BERT\n",
        "\n",
        "Useful to go in that order and see what the results are looking like\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AL6cgyxMZ3Wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instruction finetuning\n",
        "\n",
        "# finetuning a large language model pre-trained to carry out specific instructions\n",
        "\n",
        "# assume already having a pre-trained language model\n",
        "# train to create outputs ie (instruction -> inputN -> outputN) for an instruction provided by a user\n",
        "\n",
        "# instruction can be anything\n",
        "# ie write a limerick about a pelican\n",
        "# translate a text\n",
        "# summarise a text\n",
        "# perform basic math\n",
        "# any consistent type of task\n",
        "# any consistent type of question\n",
        "\n",
        "# optionally we can perform additional \"input\"\n",
        "# using the approach to do classification\n",
        "# for example\n",
        "# instruction: Identify the odd one out from the group\n",
        "# input: Carrot, Apple, Banana, Grape\n",
        "# output: Carrot\n"
      ],
      "metadata": {
        "id": "FRFBTPvAb-76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameter Efficient Finetuning: PEFT\n",
        "\n",
        "- making the supervised finetuning process a bit cheaper\n",
        "\n",
        "Methods or techniques:\n",
        "\n",
        "Classes:\n",
        "- Reparametrization based\n",
        "- Selective\n",
        "- Soft prompts\n",
        "- Adapters\n",
        "- Additive\n",
        "\n",
        "Approaches:\n",
        "\n",
        "- LoRa (Reparametrization based): most popular. Stands for Low-rank adaptation\n"
      ],
      "metadata": {
        "id": "D__S1HH0zN6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Low Rank Adaptation\n",
        "\n",
        "# Takes a large weight matrix update\n",
        "# And factorises it\n",
        "#"
      ],
      "metadata": {
        "id": "bl1Is_Y0zZ_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regular fine-tuning\n",
        "\n",
        "- pretrained weights of a model\n",
        "\n",
        "Go from inputs\n",
        "\n",
        "-> Want to produce output embeddings\n",
        "\n",
        "This involvess a large weight matrix"
      ],
      "metadata": {
        "id": "mpn9J9bb0nbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zrgPodIW0owT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}