{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshuwaifo/A-Bible-Pre-trained-Transformer-Model/blob/main/BPETokeniser_Tiktoken_BibleGPT_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Andrej Karpathy ended, Build a Large Language Model (from Scratch) book by Sebastian Raschka begins"
      ],
      "metadata": {
        "id": "jLnwkukiq4en"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz-CEIz4sWlg",
        "outputId": "ec4e9442-d8f3-475d-8755-b5c22ae67081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-12 07:51:39--  https://raw.githubusercontent.com/tushortz/variety-bible-text/master/bibles/nasb.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4685837 (4.5M) [text/plain]\n",
            "Saving to: ‘nasb.txt’\n",
            "\n",
            "nasb.txt            100%[===================>]   4.47M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-08-12 07:51:40 (69.8 MB/s) - ‘nasb.txt’ saved [4685837/4685837]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/tushortz/variety-bible-text/master/bibles/nasb.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokeniser\n",
        "\n",
        "- encode method: takes in natural text, splits it into individual tokens, converts tokens into token ID's via a vocabulary (tokenizer.encode(text))\n",
        "\n",
        "- decode method: takes in token IDs, converts token IDs into text tokens, concatenates the text tokens onto natural text (tokenizer.decode(ids))\n",
        "\n",
        "Vocabulary and Inverse vocabulary"
      ],
      "metadata": {
        "id": "8c54oUvIUW3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"nasb.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-nWlzO6kicL",
        "outputId": "1e3f6989-a1eb-40a5-fc5c-901f7d52b9d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 4623633\n",
            "In the beginning God created the heavens and the earth. -- genesis 1:1\n",
            ".\n",
            "The earth was formless and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extend vocabulary with additional special tokens\n",
        "\n",
        "Add special tokens to a vocabulary to deal with certain contexts\n",
        "\n",
        "- <|unk|> token to represent new and unknown words that were not part of the training data and thus not a part of the existing vocabulary\n",
        "\n",
        "- <|endoftext|> token to seperate two unrelated text sources"
      ],
      "metadata": {
        "id": "25F90AKuor7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Byte Pair Encoding tokenisation scheme used by Llama, GPT-3 etc.\n",
        "!pip install tiktoken\n",
        "from importlib.metadata import version\n",
        "import tiktoken\n",
        "print(\"tiktoken version:\", version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ4zeTnAmyRT",
        "outputId": "11a7ba4f-9789-4a5d-f0cf-e0ed4e7fe480"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.1 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n",
            "tiktoken version: 0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Independent text source\n",
        "\n",
        "<|endoftext|> token act as markers signalling start or end of a particular segment allowing for more effective processing and understanding by the LLM\n",
        "\n",
        "Prepend it (add it to the start) of each subsequent (following) text source\n",
        "\n",
        "Depending on the LLM tokens can be\n",
        "\n",
        "- [BOS]: Beginning of sequence\n",
        "\n",
        "- [EOS]: End of sequence\n",
        "\n",
        "- [PAD]: padding\n",
        "\n",
        "\n",
        "Byte pair encoding tokeniser doesn't use <|unk|> token"
      ],
      "metadata": {
        "id": "pvTZWsrapS6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "oxOuj5wJnKQz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BPE tokenisers break down unknown words into subwords and individual characters\n",
        "\n",
        "This allows the BPE tokeniser to parse any word and doesn't need to replace unknown words with special tokens, like <|unk|>"
      ],
      "metadata": {
        "id": "8NJgCpziqXqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert string into token ids\n",
        "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lW6JhjQnaE3",
        "outputId": "dffc08ea-96a8-40ae-909e-7c1e88d6cf22"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call the BPE tokeniser on the Word \"Akwirw ier\" and print the individual token IDs"
      ],
      "metadata": {
        "id": "8wkjnh5KrWb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resulting_integers = tokenizer.encode(\"Akwirw ier\")\n",
        "print(resulting_integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOd4SZoHrlEs",
        "outputId": "41177325-8329-4c36-923d-071dfc5fe964"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33901, 86, 343, 86, 220, 959]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call decode on resulting integers to produce a mapping of each integer (token Ids) to token texts"
      ],
      "metadata": {
        "id": "4MjrnEaNruT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# note that the input to decode has to be a list even if it is a list of one integer element\n",
        "tokens = [tokenizer.decode([integer]) for integer in resulting_integers]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQntI7rEsK5I",
        "outputId": "23c112d2-b802-4e4b-c76a-6834185e9637"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ak', 'w', 'ir', 'w', ' ', 'ier']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call decode method of token ids (resulting integers) to reconstruct orginal input"
      ],
      "metadata": {
        "id": "zhCGMawastCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_input = tokenizer.decode(resulting_integers)\n",
        "print(original_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egJavt36nlsQ",
        "outputId": "ddccaafc-d1d8-419a-a757-595277347510"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akwirw ier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "During training we mask out all words that are past the target (past the next word to ideally be predicted)\n",
        "\n"
      ],
      "metadata": {
        "id": "T6aY4D77tNLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the Bible using the BPE tokeniser\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgZ5N4KLs4U_",
        "outputId": "6484f8e3-2786-4df3-b3f8-9d02791d5df3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1249848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.249848 million tokens in the data set"
      ],
      "metadata": {
        "id": "unPhua3ityEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tensor containing the inputs: x\n",
        "# tensor containing the targets: y\n",
        "\n",
        "# use PyTorch's built in Dataset and DataLoader classes\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "#  defines how individual rows are fetched from the dataset\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        token_ids = tokenizer.encode(txt)\n",
        "\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n"
      ],
      "metadata": {
        "id": "6ZsJifvmtrJN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data loader to generate batches with input-output pairs"
      ],
      "metadata": {
        "id": "GK2d3Lc2vENC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "        stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "fpl0DCZ1u0wc"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test data loader on Bible txt"
      ],
      "metadata": {
        "id": "7CrkFIARvRPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch) # looks good"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MQHsPm9vKZ4",
        "outputId": "c6211d74-d4c2-47dc-a0af-5a8853ddc2f5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[ 818,  262, 3726, 1793]]), tensor([[ 262, 3726, 1793, 2727]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If stride is equal to input window size we can prevent overlaps between the batches\n",
        "\n",
        "A stride of 1 moves the input field by 1 position\n",
        "\n",
        "Context size = max_length here which is also equivalent somewhat to timesteps"
      ],
      "metadata": {
        "id": "OMHJMkqIvuip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing the input text for an LLM involves\n",
        "\n",
        "# Input text: \"This is an example.\"\n",
        "# tokenising text: | This | is | an | example | . |\n",
        "# converting text tokens to token IDs: | 40134 | 2052 | 133 | 389 | 12 |\n",
        "# converting token IDs into vector embedding vectors: token embedding vectors\n",
        "# creating input token embeddings\n",
        "\n",
        "# GPT-like decoder only transformer\n",
        "# Postprocessing steps\n",
        "# Output text\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GtFPY6mLwZZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example"
      ],
      "metadata": {
        "id": "C5OqdWihxVBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_token_ids = torch.tensor([2, 3, 5, 1])\n",
        "\n",
        "\n",
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "print(embedding_layer.weight) # embedding layer's underlying weight matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wAEcJnaxWBM",
        "outputId": "19f771b2-cec7-4116-e2d7-c0289b6458f2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weight matrix of embedding layer contains small random values\n",
        "\n",
        "These values are optimised during LLM training as part of the LLM optimisation itself\n",
        "\n",
        "Weight matrix has 6 rows and 3 columns\n",
        "\n",
        "One row for each of the possible 6 tokens in the vocabulary\n",
        "\n",
        "One column for each of the three embedding dimensions"
      ],
      "metadata": {
        "id": "jg2iBqtNxxBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apply embedding layer to a token id\n",
        "print(embedding_layer(torch.tensor([3])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jwuQlGLxwxv",
        "outputId": "d3cebb67-92ed-4470-e854-09ba38c7078c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is identical to the 4th row in the weight matrix above\n",
        "\n",
        "Essentially it is a look up operation that retrieves rows from the embedding layer's weight matrix via a token id"
      ],
      "metadata": {
        "id": "IHNpdiwkyOkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply embedding layer to all 4 input ids defined earlier\n",
        "print(embedding_layer(input_token_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2uqPrytxuyy",
        "outputId": "2b943965-d7df-4604-d6e2-9f3b2d5128e2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This verifies the idea of the lookup operation mentioned previously"
      ],
      "metadata": {
        "id": "5iMANkhNyun0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue from Figure 2.16"
      ],
      "metadata": {
        "id": "RgayvV2mvXEr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}